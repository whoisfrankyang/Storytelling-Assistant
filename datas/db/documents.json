[
  {
    "filename": "ATOM:_A_Framework_of_Detecting_Query-Based_Model_Extraction_Attacks_for_Graph_Neural_Networks_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nATOM: A Framework of Detecting Query-Based Model Extraction\nAttacks for Graph Neural Networks\nZhan Cheng\nUniversity of Wisconsin, Madison\nMadison, Wisconsin, USA\nzcheng256@wisc.eduBolin Shen\nFlorida State University\nTallahassee, Florida, USA\nblshen@fsu.eduTianming Sha\nArizona State University\nTempe, Arizona, USA\nstianmin@asu.edu\nYuan Gao\nUniversity of Wisconsin, Madison\nMadison, Wisconsin, USA\nygao355@wisc.eduShibo Li\nFlorida State University\nTallahassee, Florida, USA\nsl24bp@fsu.eduYushun Dong\nFlorida State University\nTallahassee, Florida, USA\nyushun.dong@fsu.edu\n\n=== ABSTRACT ===\n\nGraph Neural Networks (GNNs) have gained traction in Graph-\nbased Machine Learning as a Service (GMLaaS) platforms, yet they\nremain vulnerable to graph-based model extraction attacks (MEAs),\nwhere adversaries reconstruct surrogate models by querying the vic-\ntim model. Existing defense mechanisms, such as watermarking and\nfingerprinting, suffer from poor real-time performance, susceptibil-\nity to evasion, or reliance on post-attack verification, making them\ninadequate for handling the dynamic characteristics of graph-based\nMEA variants. To address these limitations, we propose ATOM,\na novel real-time MEA detection framework tailored for GNNs.\nATOM integrates sequential modeling and reinforcement learning\nto dynamically detect evolving attack patterns, while leveraging\n\ud835\udc58-core embedding to capture the structural properties, enhancing\ndetection precision. Furthermore, we provide theoretical analysis\nto characterize query behaviors and optimize detection strategies.\nExtensive experiments on multiple real-world datasets demonstrate\nthat ATOM outperforms existing approaches in detection perfor-\nmance, maintaining stable across different time steps, thereby offer-\ning a more effective defense mechanism for GMLaaS environments.\nOur source code is available at https://github.com/LabRAI/ATOM.\nKeywords\nGraph Neural Networks, Model Extraction Attacks, Machine Learn-\ning as a Service, Security"
  },
  {
    "filename": "EDiT:_Efficient_Diffusion_Transformers_with_Linear_Compressed_Attention_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nEDiT: Efficient Diffusion Transformers with Linear Compressed Attention\nPhilipp Becker1,2Abhinav Mehrotra1Ruchika Chavhan1Malcolm Chadwick1\nLuca Morreale1Mehdi Noroozi1Alberto Gil Ramos1Sourav Bhattacharya1\n1Samsung AI Center, Cambridge2Karlsruhe Institute of Technology\n\n=== ABSTRACT ===\n\nDiffusion Transformers (DiTs) have emerged as a lead-\ning architecture for text-to-image synthesis, producing\nhigh-quality and photorealistic images. However, the\nquadratic scaling properties of the attention in DiTs hin-\nder image generation with higher resolution or on devices\nwith limited resources. This work introduces an efficient dif-\nfusion transformer (EDiT) to alleviate these efficiency bot-\ntlenecks in conventional DiTs and Multimodal DiTs (MM-\nDiTs). First, we present a novel linear compressed atten-\ntion method that uses a multi-layer convolutional network\nto modulate queries with local information while keys and\nvalues are spatially aggregated. Second, we formulate a\nhybrid attention scheme for multi-modal inputs that com-\nbines linear attention for image-to-image interactions and\nstandard scaled dot-product attention for interactions in-\nvolving prompts. Merging these two approaches leads to\nan expressive, linear-time Multimodal Efficient Diffusion\nTransformer (MM-EDiT). We demonstrate the effectiveness\nof the EDiT and MM-EDiT architectures by integrating\nthem into PixArt- \u03a3(conventional DiT) and Stable Diffusion\n3.5-Medium (MM-DiT), achieving up to 2.2\u00d7speedup with\ncomparable image quality after distillation."
  },
  {
    "filename": "Glivenko-Cantelli_for_$f$-divergence_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nGLIVENKO\u2013CANTELLI FOR f-DIVERGENCE\nHAOMING WANG AND LEK-HENG LIM\n\n=== ABSTRACT ===\n\n. We extend the celebrated Glivenko\u2013Cantelli theorem, sometimes called the fundamen-\ntal theorem of statistics, from its standard setting of total variation distance to all f-divergences. A\nkey obstacle in this endeavor is to define f-divergence on a subcollection of a \u03c3-algebra that forms\na\u03c0-system but not a \u03c3-subalgebra. This is a side contribution of our work. We will show that this\nnotion of f-divergence on the \u03c0-system of rays preserves nearly all known properties of standard\nf-divergence, yields a novel integral representation of the Kolmogorov\u2013Smirnov distance, and has\na Glivenko\u2013Cantelli theorem. We will also discuss the prospects of a Vapnik\u2013Chervonenkis theory\nforf-divergence."
  },
  {
    "filename": "FactSelfCheck:_Fact-Level_Black-Box_Hallucination_Detection_for_LLMs_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nFactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs\nAlbert Sawczyn1Jakub Binkowski1Denis Janiak1\nBogdan Gabrys2Tomasz Kajdanowicz1\n1Wroc\u0142aw University of Science and Technology\n2University of Technology Sydney\nalbert.sawczyn@pwr.edu.pl\n\n=== ABSTRACT ===\n\nLarge Language Models (LLMs) frequently\ngenerate hallucinated content, posing signif-\nicant challenges for applications where fac-\ntuality is crucial. While existing hallucina-\ntion detection methods typically operate at the\nsentence level or passage level, we propose\nFactSelfCheck, a novel black-box sampling-\nbased method that enables fine-grained fact-\nlevel detection. Our approach represents text\nas knowledge graphs consisting of facts in the\nform of triples. Through analyzing factual\nconsistency across multiple LLM responses,\nwe compute fine-grained hallucination scores\nwithout requiring external resources or train-\ning data. Our evaluation demonstrates that\nFactSelfCheck performs competitively with\nleading sampling-based methods while provid-\ning more detailed insights. Most notably, our\nfact-level approach significantly improves hal-\nlucination correction, achieving a 35% increase\nin factual content compared to the baseline,\nwhile sentence-level SelfCheckGPT yields only\nan 8% improvement. The granular nature of\nour detection enables more precise identifica-\ntion and correction of hallucinated content."
  },
  {
    "filename": "A_New_Statistical_Model_of_Star_Speckles_for_Learning_to_Detect_and_Characterize_Exoplanets_in_Direct_Imaging_Observations_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nA New Statistical Model of Star Speckles for Learning to Detect and\nCharacterize Exoplanets in Direct Imaging Observations\nTh\u00b4eo Bodrito*1, Olivier Flasseur2, Julien Mairal3, Jean Ponce1, 4,\nMaud Langlois2, Anne-Marie Lagrange5, 6\n1D\u00b4epartement d\u2019Informatique de l\u2019 \u00b4Ecole normale sup \u00b4erieure (ENS-PSL, CNRS, Inria)\n2Universite Claude Bernard Lyon 1, Centre de Recherche Astrophysique de Lyon UMR 5574,\nENS de Lyon, CNRS, Villeurbanne, F-69622, France\n3Universit \u00b4e Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK\n4Courant Institute and Center for Data Science, New York University\n5Laboratoire d\u2019 \u00b4Etudes Spatiales et d\u2019Instrumentation en Astrophysique, Observatoire de Paris,\nUniversit \u00b4e PSL, Sorbonne Universit \u00b4e, Universit \u00b4e Paris Diderot\n6Universit \u00b4e Grenoble Alpes, Institut de Plan \u00b4etologie et d\u2019Astrophysique de Grenoble\n\n=== ABSTRACT ===\n\nThe search for exoplanets is an active field in astronomy,\nwith direct imaging as one of the most challenging meth-\nods due to faint exoplanet signals buried within stronger\nresidual starlight. Successful detection requires advanced\nimage processing to separate the exoplanet signal from this\nnuisance component. This paper presents a novel statisti-\ncal model that captures nuisance fluctuations using a multi-\nscale approach, leveraging problem symmetries and a joint\nspectral channel representation grounded in physical prin-\nciples. Our model integrates into an interpretable, end-to-\nend learnable framework for simultaneous exoplanet detec-\ntion and flux estimation. The proposed algorithm is eval-\nuated against the state of the art using datasets from the\nSPHERE instrument operating at the Very Large Telescope\n(VLT). It significantly improves the precision-recall trade-\noff, notably on challenging datasets that are otherwise un-\nusable by astronomers. The proposed approach is compu-\ntationally efficient, robust to varying data quality, and well\nsuited for large-scale observational surveys.1"
  },
  {
    "filename": "A_Learnability_Analysis_on_Neuro-Symbolic_Learning_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nA Learnability Analysis on Neuro-Symbolic Learning\nHao-Yuan He ,Ming Li\nNational Key Laboratory for Novel Software Technology, Nanjing University, China\nSchool of Artificial Intelligence, Nanjing University, China\n{hehy,lim }@lamda.nju.edu.cn\n\n=== ABSTRACT ===\n\nThis paper analyzes the learnability of neuro-symbolic (NeSy) tasks in hybrid systems, such as probabilistic\nNeSy methods and abductive learning. We demonstrate that the learnability of NeSy tasks can be characterized\nby their derived constraint satisfaction problems (DCSPs). Specifically, a task is learnable if the corresponding\nDCSP has a unique solution; otherwise, it is unlearnable . For learnable tasks, we derive the corresponding\nsample complexity. For general tasks, we establish the asymptotic error and demonstrate that the expected error\nscales with the disagreement among solutions. Our results offer a principled approach to determining learnability\nand provide insights for new algorithms."
  },
  {
    "filename": "Unitless_Unrestricted_Markov-Consistent_SCM_Generation:_Better_Benchmark_Datasets_for_Causal_Discovery_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nProceedings of Machine Learning Research TBD:1\u201326, 2025 4th Conference on Causal Learning and Reasoning\nUnitless Unrestricted Markov-Consistent SCM Generation:\nBetter Benchmark Datasets for Causal Discovery\nRebecca J. Herman REBECCA .HERMAN @TU-DRESDEN .DE\nCenter for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI), Dresden/Leipzig, Germany,\nGerman Aerospace Center (DLR), Institute of Data Science, Jena, Germany\nJonas Wahl JONAS .WAHL @DFKI .DE\nGerman Research Centre for Artificial Intelligence (DFKI), Berlin, Germany\nUrmi Ninad URMI .NINAD @TU-BERLIN .DE\nBerlin Institute of Technology (TUB), Institute of Computer Engineering and Microelectronics, Germany\nJakob Runge JAKOB .RUNGE @TU-DRESDEN .DE\nCenter for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI), Dresden/Leipzig, Germany\nBerlin Institute of Technology (TUB), Institute of Computer Engineering and Microelectronics, Germany\nEditors: Biwei Huang and Mathias Drton\n\n=== ABSTRACT ===\n\nCausal discovery aims to extract qualitative causal knowledge in the form of causal graphs from\ndata. Because causal ground truth is rarely known in the real world, simulated data plays a vital role\nin evaluating the performance of the various causal discovery algorithms proposed in the literature.\nBut recent work highlighted certain artifacts of commonly used data generation techniques for a\nstandard class of structural causal models (SCM) that may be nonphysical, including var- and R2-\nsortability, where the variables\u2019 variance and coefficients of determination (R2) after regressing\non all other variables, respectively, increase along the causal order. Some causal methods exploit\nsuch artifacts, leading to unrealistic expectations for their performance on real-world data. Some\nmodifications have been proposed to remove these artifacts; notably, the internally-standardized\nstructural causal model (iSCM) avoids varsortability and largely alleviates R2-sortability on sparse\ncausal graphs, but exhibits a reversed R2-sortability pattern for denser graphs not featured in their\nwork. We analyze which sortability patterns we expect to see in real data, and propose a method\nfor drawing coefficients that we argue more effectively samples the space of SCMs. Finally, we\npropose a novel extension of our SCM generation method to the time series setting."
  },
  {
    "filename": "Offline_Model-Based_Optimization:_Comprehensive_Review_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nOffline Model-Based Optimization: Comprehensive Review\nMinsu Kim\u2217minsu.kim@mila.quebec\nMila - Quebec AI Institute/KAIST\nJiayao (Claris) Gu\u2217jia.yao.gu@mail.mcgill.ca\nMila - Quebec AI Institute/McGill University\nYe Yuan ye.yuan3@mail.mcgill.ca\nMila - Quebec AI Institute/McGill University\nTaeyoung Yun taeyoung.yun@mila.quebec\nMila - Quebec AI Institute/KAIST\nZixuan Liu zucksliu@cs.washington.edu\nUniversity of Washington\nYoshua Bengio yoshua.bengio@mila.quebec\nMila - Quebec AI Institute/University of Montreal\nCan (Sam) Chen\u2020can.chen@mila.quebec\nMila - Quebec AI Institute/University of Montreal\n\n=== ABSTRACT ===\n\nOffline optimization is a fundamental challenge in science and engineering, where the goal is to\noptimize black-box functions using only offline datasets. This setting is particularly relevant\nwhen querying the objective function is prohibitively expensive or infeasible, with applications\nspanning protein engineering, material discovery, neural architecture search, and beyond. The\nmain difficulty lies in accurately estimating the objective landscape beyond the available data,\nwhere extrapolations are fraught with significant epistemic uncertainty. This uncertainty\ncan lead to objective hacking (reward hacking )\u2014exploiting model inaccuracies in unseen\nregions\u2014or other spurious optimizations that yield misleadingly high performance estimates\noutside the training distribution. Recent advances in model-based optimization (MBO) have\nharnessed the generalization capabilities of deep neural networks to develop offline-specific\nsurrogate and generative models. Trained with carefully designed strategies, these models are\nmore robust against out-of-distribution issues, facilitating the discovery of improved designs.\nDespite its growing impact in accelerating scientific discovery, the field lacks a comprehensive\nreview. To bridge this gap, we present the first thorough review of offline MBO. We begin\nby formalizing the problem for both single-objective andmulti-objective settings and by\nreviewing recent benchmarks and evaluation metrics. We then categorize existing approaches\ninto two key areas: surrogate modeling , which emphasizes accurate function approximation\nin out-of-distribution regions, and generative modeling , which explores high-dimensional\ndesign spaces to identify high-performing designs. Finally, we examine the key challenges\nand propose promising directions for advancement in this rapidly evolving field including safe\ncontrol of superintelligent systems. For a curated list of resources, please visit our repository.\n\u2217Minsu and Claris contributed equally to this work.\n\u2020Correspondence: can.chen@mila.quebec or chencan421@gmail.com\n1arXiv:2503.17286v1  [cs.LG]  21 Mar 2025"
  },
  {
    "filename": "Benign_Overfitting_with_Quantum_Kernels_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nBenign Overfitting with Quantum Kernels\nJoachim Tomasi1,2, Sandrine Anthoine2, and Hachem Kadri1\n1Aix-Marseille University, CNRS, LIS, Marseille, France\n2Aix-Marseille University, CNRS, I2M, Marseille, France\n{firstname.lastname }@univ-amu.fr\n\n=== ABSTRACT ===\n\nQuantum kernels quantify similarity between data points by measuring the inner product\nbetween quantum states, computed through quantum circuit measurements. By embedding\ndata into quantum systems, quantum kernel feature maps, that may be classically intractable\nto compute, could efficiently exploit high-dimensional Hilbert spaces to capture complex pat-\nterns. However, designing effective quantum feature maps remains a major challenge. Many\nquantum kernels, such as the fidelity kernel, suffer from exponential concentration, leading\nto near-identity kernel matrices that fail to capture meaningful data correlations and lead\nto overfitting and poor generalization. In this paper, we propose a novel strategy for con-\nstructing quantum kernels that achieve good generalization performance, drawing inspiration\nfrom benign overfitting in classical machine learning. Our approach introduces the concept\nof local-global quantum kernels, which combine two complementary components: a local\nquantum kernel based on measurements of small subsystems and a global quantum kernel\nderived from full-system measurements. Through numerical experiments, we demonstrate\nthat local-global quantum kernels exhibit benign overfitting, supporting the effectiveness of\nour approach in enhancing quantum kernel methods."
  },
  {
    "filename": "Universal_approximation_property_of_neural_stochastic_differential_equations_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\narXiv:2503.16696v1  [math.PR]  20 Mar 2025UNIVERSAL APPROXIMATION PROPERTY OF NEURAL STOCHASTIC\nDIFFERENTIAL EQUATIONS\nANNA P. KWOSSEK, DAVID J. PR \u00a8OMEL, AND JOSEF TEICHMANN\n\n=== ABSTRACT ===\n\n. We identify various classes of neural networks that are able to approximate\ncontinuous functions locally uniformly subject to \ufb01xed glo bal linear growth constraints. For\nsuch neural networks the associated neural stochastic di\ufb00e rential equations can approximate\ngeneral stochastic di\ufb00erentialequations, bothofIt\u02c6 odi\ufb00 usion type, arbitrarily well. Moreover,\nquantitative error estimates are derived for stochastic di \ufb00erential equations with su\ufb03ciently\nregular coe\ufb03cients.\nKey words: feedforward neural network, linear growth, neural stochas tic di\ufb00erential equa-\ntion, universal approximation theorem, ReLU activation fu nction, weighted function space.\nMSC 2020 Classi\ufb01cation: 41A29, 60H10, 68T07, 91G80."
  },
  {
    "filename": "QCPINN:_Quantum_Classical_Physics-Informed_Neural_Networks_for_Solving_PDEs_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nQCPINN: Quantum Classical Physics-Informed Neural Networks for\nSolving PDEs\nAfrah Fareaa, Saiful Khanb, Mustafa Serdar Celebia\naInformatics Institute, Department of Computational Science and Engineering, Istanbul Technical\nUniversity, Istanbul34469 Turkey\nbScientific Computing Department, Rutherford Appleton Laboratory, Science and Technology Facilities Council\n(STFC), United Kingdom\n\n=== ABSTRACT ===\n\nHybrid quantum-classical neural network methods represent an emerging approach to solving com-\nputationally challenging differential equations by leveraging advantages from both paradigms. While\nphysics-informed neural networks (PINNs) have successfully orporated physical constraints into neural\narchitectures for solving partial differential equations (PDEs), the potential quantum advantages in this\ndomain remain largely unexplored. This work investigates whether quantum-classical physics-informed\nneural networks (QCPINNs) can efficiently solve PDEs with reduced parameter counts compared to\nclassical approaches. We systematically evaluate two quantum circuit paradigms: continuous-variable\n(CV) and qubit-based discrete-variable (DV) implementations across multiple circuit ans\u00a8 atze (Alter-\nnate, Cascade, Cross-mesh, and Layered). Studies across five challenging PDEs (Helmholtz, Cavity,\nWave, Klein-Gordon, and Convection-Diffusion equations) demonstrate that our hybrid approaches\nachieve comparable accuracy to classical PINNs while requiring up to 89% fewer trainable parameters.\nDV-based implementations, particularly those with angle encoding and cascade circuit configurations,\nexhibit better stability and convergence properties across all problem types. For the Convection-\nDiffusion equation, our angle-cascade QCPINN achieves 10x parameter efficiency and a 43% reduction\nin relative L2error compared to classical counterparts. Our findings highlight the potential of quantum-\nenhanced architectures for physics-informed learning, establishing parameter efficiency as a quantifiable\nquantum advantage while providing a foundation for future quantum-classical hybrid systems solving\ncomplex physical models."
  },
  {
    "filename": "Physics-Informed_Deep_B-Spline_Networks_for_Dynamical_Systems_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\n[Distribution A: Approved for public release; distribution is unlimited.]\nPhysics-Informed Deep B-Spline Networks for Dynamical Systems\nZhuoyuan Wang1Raffaele Romagnoli2Jasmine Ratchford3Yorie Nakahira1\n\n=== ABSTRACT ===\n\nPhysics-informed machine learning provides an\napproach to combining data and governing\nphysics laws for solving complex partial differen-\ntial equations (PDEs). However, efficiently solv-\ning PDEs with varying parameters and chang-\ning initial conditions and boundary conditions\n(ICBCs) with theoretical guarantees remains an\nopen challenge. We propose a hybrid frame-\nwork that uses a neural network to learn B-spline\ncontrol points to approximate solutions to PDEs\nwith varying system and ICBC parameters. The\nproposed network can be trained efficiently as\none can directly specify ICBCs without imposing\nlosses, calculate physics-informed loss functions\nthrough analytical formulas, and requires only\nlearning the weights of B-spline functions as op-\nposed to both weights and basis as in traditional\nneural operator learning methods. We provide\ntheoretical guarantees that the proposed B-spline\nnetworks serve as universal approximators for the\nset of solutions of PDEs with varying ICBCs un-\nder mild conditions and establish bounds on the\ngeneralization errors in physics-informed learn-\ning. We also demonstrate in experiments that the\nproposed B-spline network can solve problems\nwith discontinuous ICBCs and outperforms exist-\ning methods, and is able to learn solutions of 3D\ndynamics with diverse initial conditions."
  },
  {
    "filename": "Deterministic_AI_Agent_Personality_Expression_through_Standard_Psychological_Diagnostics_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nAllora Decentralized Intelligence 2, 15\u201339; 2025 March 24 doi:10.70235/allora.0x20015\nDeterministic AI Agent Personality Expression\nthrough Standard Psychological Diagnostics\nJ. M. Diederik Kruijssen & Nicholas Emmons\nAllora Foundation\n\n=== ABSTRACT ===\n\nArtificial intelligence (AI) systems powered by large language models have become increasingly prevalent in modern\nsociety, enabling a wide range of applications through natural language interaction. As AI agents proliferate in our daily\nlives, their generic and uniform expressiveness presents a significant limitation to their appeal and adoption. Personality\nexpression represents a key prerequisite for creating more human-like and distinctive AI systems. We show that AI mod-\nels can express deterministic and consistent personalities when instructed using established psychological frameworks,\nwith varying degrees of accuracy depending on model capabilities. We find that more advanced models like GPT-4o and\no1 demonstrate the highest accuracy in expressing specified personalities across both Big Five and Myers-Briggs assess-\nments, and further analysis suggests that personality expression emerges from a combination of intelligence and reasoning\ncapabilities. Our results reveal that personality expression operates through holistic reasoning rather than question-by-\nquestion optimization, with response-scale metrics showing higher variance than test-scale metrics. Furthermore, we find\nthat model fine-tuning affects communication style independently of personality expression accuracy. These findings\nestablish a foundation for creating AI agents with diverse and consistent personalities, which could significantly enhance\nhuman-AI interaction across applications from education to healthcare, while additionally enabling a broader range of\nmore unique AI agents. The ability to quantitatively assess and implement personality expression in AI systems opens\nnew avenues for research into more relatable, trustworthy, and ethically designed AI."
  },
  {
    "filename": "Sparse_Additive_Contextual_Bandits:_A_Nonparametric_Approach_for_Online_Decision-making_with_High-dimensional_Covariates_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nSparse Additive Contextual Bandits: A Nonparametric\nApproach for Online Decision-making with\nHigh-dimensional Covariates\nWenjia Wang\u2217\nData Science and Analytics Thrust,\nThe Hong Kong University of Science and Technology (Guangzhou)\nDepartment of Mathematics,\nThe Hong Kong University of Science and Technology\nQingwen Zhang\u2217 \u2020\nDivision of Emerging Interdisciplinary Areas,\nThe Hong Kong University of Science and Technology\nXiaowei Zhang\u2217\nDepartment of Industrial Engineering and Decision Analytics,\nThe Hong Kong University of Science and Technology\n\n=== ABSTRACT ===\n\nPersonalized services are central to today\u2019s digital landscape, where online decision-\nmaking is commonly formulated as contextual bandit problems. Two key challenges\nemerge in modern applications: high-dimensional covariates and the need for nonpara-\nmetric models to capture complex reward-covariate relationships. We address these\nchallenges by developing a contextual bandit algorithm based on sparse additive reward\nmodels in reproducing kernel Hilbert spaces. We establish statistical properties of the\ndoubly penalized method applied to random regions, introducing novel analyses under\nbandit feedback. Our algorithm achieves sublinear cumulative regret over the time\nhorizon Twhile scaling logarithmically with covariate dimensionality d. Notably, we\nprovide the first regret upper bound with logarithmic growth in dfor nonparametric\ncontextual bandits with high-dimensional covariates. We also establish a lower bound,\nwith the gap to the upper bound vanishing as smoothness increases. Extensive numerical\nexperiments demonstrate our algorithm\u2019s superior performance in high-dimensional\nsettings compared to existing approaches.\nKeywords : High-dimensional contextual bandits; nonparametric regression; kernel methods;\nreproducing kernel Hilbert space\n\u2217These authors contributed equally to this work.\n\u2020Corresponding author: qzhangcg@connect.ust.hk\n1arXiv:2503.16941v1  [stat.ML]  21 Mar 2025"
  },
  {
    "filename": "Deep_Q-Learning_with_Gradient_Target_Tracking_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nDeep Q-Learning with Gradient Target Tracking\nDonghwan Lee\nElectrical Engineering, KAIST\ndonghwan@kaist.ac.krBum Geun Park\nElectrical Engineering, KAIST\nj4t123@kaist.ac.kr\nTaeho Lee\nElectrical Engineering, KAIST\neho0228@kaist.ac.kr\n\n=== ABSTRACT ===\n\nThis paper introduces Q-learning with gradient target tracking, a novel reinforce-\nment learning framework that provides a learned continuous target update mecha-\nnism as an alternative to the conventional hard update paradigm. In the standard\ndeep Q-network (DQN), the target network is a copy of the online network\u2019s\nweights, held fixed for a number of iterations before being periodically replaced via\na hard update. While this stabilizes training by providing consistent targets, it intro-\nduces a new challenge: the hard update period must be carefully tuned to achieve\noptimal performance. To address this issue, we propose two gradient-based target\nupdate methods: DQN with asymmetric gradient target tracking (AGT2-DQN)\nand DQN with symmetric gradient target tracking (SGT2-DQN). These methods\nreplace the conventional hard target updates with continuous and structured updates\nusing gradient descent, which effectively eliminates the need for manual tuning.\nWe provide a theoretical analysis proving the convergence of these methods in\ntabular settings. Additionally, empirical evaluations demonstrate their advantages\nover standard DQN baselines, which suggest that gradient-based target updates\ncan serve as an effective alternative to conventional target update mechanisms in\nQ-learning."
  },
  {
    "filename": "CAARMA:_Class_Augmentation_with_Adversarial_Mixup_Regularization_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nCAARMA: Class Augmentation with Adversarial Mixup Regularization\nMassa Baali, Xiang Li, Hao Chen, Rita Singh, Bhiksha Raj\nCarnegie Mellon University\nmbaali@cs.cmu.edu\n\n=== ABSTRACT ===\n\nSpeaker verification is a typical zero-shot learn-\ning task, where inference of unseen classes is\nperformed by comparing embeddings of test\ninstances to known examples. The models per-\nforming inference must hence naturally gen-\nerate embeddings that cluster same-class in-\nstances compactly, while maintaining separa-\ntion across classes. In order to learn to do so,\nthey are typically trained on a large number\nof classes (speakers), often using specialized\nlosses. However real-world speaker datasets of-\nten lack the class diversity needed to effectively\nlearn this in a generalizable manner. We intro-\nduce CAARMA, a class augmentation frame-\nwork that addresses this problem by generating\nsynthetic classes through data mixing in the\nembedding space, expanding the number of\ntraining classes. To ensure the authenticity of\nthe synthetic classes we adopt a novel adversar-\nial refinement mechanism that minimizes cate-\ngorical distinctions between synthetic and real\nclasses. We evaluate CAARMA on multiple\nspeaker verification tasks, as well as other rep-\nresentative zero-shot comparison-based speech\nanalysis tasks and obtain consistent improve-\nments: our framework demonstrates a signifi-\ncant improvement of 8% over all baseline mod-\nels. Code for CAARMA will be released."
  },
  {
    "filename": "Revisiting_End_To_End_Sparse_Autoencoder_Training_--_A_Short_Finetune_is_All_You_Need_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nRevisiting End To End Sparse Autoencoder Training - A Short Finetune is All\nYou Need\nAdam Karvonen1\n\n=== ABSTRACT ===\n\nSparse autoencoders (SAEs) are widely used for\ninterpreting language model activations. A key\nevaluation metric is the increase in cross-entropy\nloss when replacing model activations with SAE\nreconstructions. Typically, SAEs are trained\nsolely on mean squared error (MSE) using pre-\ncomputed, shuffled activations. Recent work intro-\nduced training SAEs directly with a combination\nof KL divergence and MSE (\u201cend-to-end\u201d SAEs),\nsignificantly improving reconstruction accuracy\nat the cost of substantially increased computa-\ntion, which has limited their widespread adoption.\nWe propose a brief KL+MSE fine-tuning step ap-\nplied only to the final 25M training tokens (just\na few percent of typical training budgets) that\nachieves comparable improvements, reducing the\ncross-entropy loss gap by 20\u201350%, while incur-\nring minimal additional computational cost. We\nfurther find that multiple fine-tuning methods (KL\nfine-tuning, LoRA adapters, linear adapters) yield\nsimilar, non-additive cross-entropy improvements,\nsuggesting a common, easily correctable error\nsource in MSE-trained SAEs. We demonstrate\na straightforward method for effectively trans-\nferring hyperparameters and sparsity penalties\ndespite scale differences between KL and MSE\nlosses. While both ReLU and TopK SAEs see\nsignificant cross-entropy loss improvements, eval-\nuations on supervised SAEBench metrics yield\nmixed results, suggesting practical benefits de-\npend on both SAE architecture and the specific\ndownstream task. Nonetheless, our method of-\nfers meaningful improvements in interpretability\napplications such as circuit analysis with minor\nadditional cost.1\n1Independent. Correspondence to: Adam Karvonen\n<adam.karvonen@gmail.com >.\n1Code and models available at: https://github.com/\nadamkarvonen/sae_kl_finetune1. Introduction\nSparse autoencoders (SAEs) have emerged as an important\ntool in mechanistic interpretability, enabling the decomposi-\ntion of language model activations into sparse linear combi-\nnations of interpretable latent features (Cunningham et al.,\n2023; Bricken et al., 2023). The central hypothesis behind\nSAEs is that neural activations can be effectively represented\nusing sparse combinations of meaningful latent directions,\nfacilitating deeper understanding and interpretability of neu-\nral network computations.\nThe primary evaluation metric for SAEs is the increase\nin cross-entropy loss incurred when original model acti-\nvations are replaced by SAE reconstructions during infer-\nence, capturing the trade-off between sparsity and fidelity.\nRecent advances have focused extensively on improving\nthis sparsity-fidelity trade-off through novel SAE architec-\ntures (Rajamanoharan et al., 2024a; Mudide et al., 2024),\nactivation functions (Gao et al., 2024; Taggart, 2024; Raja-\nmanoharan et al., 2024b; Bussmann et al., 2024a; Ayonrinde,\n2024), and training loss formulations (Karvonen et al., 2024;\nBussmann et al., 2024b).\nTypically, SAEs are trained with mean squared error\n(MSE) loss on precomputed and shuffled model activations\n(Lieberum et al., 2024). However, this approach does not\ndirectly optimize the cross-entropy loss used during evalu-\nation. Recent methods have explored training SAEs with\na combination of KL divergence and MSE loss to align\ntraining objectives more closely with evaluation objectives.\nNotably, Braun et al. (2024) proposed \u201dend-to-end\u201d SAE\ntraining, using KL+MSE loss throughout training. While\neffective, this method substantially increases computational\ncost, thus limiting practical applicability and widespread\nadoption by the field.\nAlternatively, Chen et al. (2025) introduced a method lever-\naging low-rank adapters (LoRA) to fine-tune the underlying\nlanguage model around pre-trained SAEs. This approach\nachieves similar performance gains with much lower com-\nputational overhead. However, it introduces additional com-\nplexity and alters the original language model, which may\nbe undesirable for interpretability studies.\nIn this work, we revisit end-to-end SAE training and demon-\n1arXiv:2503.17272v1  [cs.LG]  21 Mar 2025\nRevisiting End To End Sparse Autoencoder Training - A Short Finetune is All You Need\nFigure 1: Comparison of training approaches for a sparse autoencoder (K=80, width=16K) on Pythia-160M. The proposed\nKL+MSE fine-tuning approach (25M tokens) achieves slightly better performance than full end-to-end (E2E) training\n(Braun et al., 2024) on the same amount of data while reducing wall-clock time by approximately 50%.\nstrate a simpler yet equally effective strategy: applying a\nbrief fine-tuning stage with KL+MSE loss for only the final\n25M tokens of training (0.5-10% of typical training budgets\n(250M - 5B tokens)). In our particular setting, this targeted\napproach exceeds the performance of both full end-to-end\ntraining and LoRA adapters, while significantly reducing\nwall-clock time\u2014approximately 50% reduction compared\nto full end-to-end training, and a minor reduction compared\nto LoRA adapters. In other common scenarios, such as train-\ning on larger language models, employing early stopping\nduring activation collection, or amortizing SAE training\nacross precomputed activations, wall-clock time savings\nrelative to end-to-end training can easily exceed 90%.2\nThese results suggest that training with an MSE loss func-\ntion learns meaningful features with an easily correctable\nKL divergence error source which can be removed by multi-\nple methods, of which SAE fine-tuning is the simplest and\nmost performant. We present a practical recipe for smoothly\ntransferring hyperparameters and sparsity penalties from\nMSE-only to KL+MSE training phases, addressing chal-\nlenges arising from significant scale differences between\nthese losses.\n2For example, training six SAEs at the middle layer involves\nKL divergence loss (two forward passes + one backward, 4 \u00d7cost),\nno early stopping (2 \u00d7), and no amortization across SAEs (6 \u00d7),\ntotaling up to 48 \u00d7extra compute for activation collection. Actual\nwall-clock savings depend on SAE-to-LLM size ratios.We evaluate our proposed method on SAEBench (Karvonen\net al., 2025), a comprehensive suite of metrics beyond recon-\nstruction accuracy. Evaluations on downstream supervised\nmetrics yield mixed results, showing both improvements and\ndeclines depending on both the specific evaluation metric\nand SAE architecture, with particularly pronounced changes\nfor ReLU-based SAEs. Despite this, we believe that achiev-\ning considerable reductions in cross-entropy loss at minimal\ncost can substantially benefit interpretability-focused appli-\ncations, such as circuit analysis, by reducing the influence\nof reconstruction error nodes and thus decreasing the risk\nof missing critical signals.\nOur main contributions include:\n1.A simplified and computationally efficient fine-tuning\napproach for achieving end-to-end SAE training bene-\nfits without altering model architecture or adding sig-\nnificant complexity.\n2.A practical method for transferring hyperparameters\nand sparsity penalties between MSE and KL+MSE\ntraining phases.\n3.Empirical evidence demonstrating mixed results in su-\npervised interpretability metrics on SAEBench, high-\nlighting that interpretability benefits from KL+MSE\ntraining depend on both SAE architecture and the\ndownstream task.\n2\nRevisiting End To End Sparse Autoencoder Training - A Short Finetune is All You Need"
  },
  {
    "filename": "Making_the_unmodulated_pyramid_wavefront_sensor_smart_II._First_on-sky_demonstration_of_extreme_adaptive_optics_with_deep_learning_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nAstronomy &Astrophysics manuscript no. main \u00a9ESO 2025\nMarch 24, 2025\nLetter to the Editor\nMaking the unmodulated pyramid wavefront sensor smart\nII. First on-sky demonstration of extreme adaptive optics with deep learning\nR. Landman1, S. Y . Ha ffert1,2, J. D. Long3, J. R. Males2, L. M. Close2, W. B. Foster2, K. Van Gorkom2, O.\nGuyon2,4,5,6, A. D. Hedglen7, P. T. Johnson2, M. Y . Kautz4, J. K. Kueny4, J. Li2, J. Liberman4, J. Lumbres4,\nE. A. McEwen4, A. McLeod8, L. Schatz9, E. Tonucci1, and K. Twitchell4\n1Leiden Observatory, Leiden University, PO Box 9513, 2300 RA Leiden, The Netherlands\ne-mail: rlandman@strw.leidenuniv.nl\n2Steward Observatory, The Unversity of Arizona, 933 North Cherry Avenue, Tucson, Arizona\n3Center for Computational Astrophysics, Flatiron Institute, 162 5th Avenue, New York, New York\n4Wyant College of Optical Sciences, The University of Arizona, 1630 E University Blvd, Tucson, Arizona\n5Subaru Telescope, National Observatory of Japan, National Institutes of Natural Sciences, 650 N. A\u2019ohoku Place, Hilo, Hawai\u2019i\n6Astrobiology Center, National Institutes of Natural Sciences, 2-21-1 Osawa, Mitaka, Tokyo, Japan\n7Northrop Grumman Corporation, 600 South Hicks Road, Rolling Meadows, Illinois\n8Draper Laboratory, 555 Technology Square, Cambridge, Massachusetts\n9Starfire Optical Range, Kirtland Air Force Base, Albuquerque, New Mexico\nReceived ; accepted\n\n=== ABSTRACT ===\n\nPyramid wavefront sensors (PWFSs) are the preferred choice for current and future extreme adaptive optics (XAO) systems. Almost\nall instruments use the PWFS in its modulated form to mitigate its limited linearity range. However, this modulation comes at the\ncost of a reduction in sensitivity, a blindness to petal-piston modes, and a limit to the sensor\u2019s ability to operate at high speeds.\nTherefore, there is strong interest to use the PWFS without modulation, which can be enabled with nonlinear reconstructors. Here, we\npresent the first on-sky demonstration of XAO with an unmodulated PWFS using a nonlinear reconstructor based on convolutional\nneural networks. We discuss the real-time implementation on the Magellan Adaptive Optics eXtreme (MagAO-X) instrument using\nthe optimized TensorRT framework and show that inference is fast enough to run the control loop at >2 kHz frequencies. Our on-sky\nresults demonstrate a successful closed-loop operation using a model calibrated with internal source data that delivers stable and robust\ncorrection under varying conditions. Performance analysis reveals that our smart PWFS achieves nearly the same Strehl ratio as the\nhighly optimized modulated PWFS under favorable conditions on bright stars. Notably, we observe an improvement in performance\non a fainter star under the influence of strong winds. These findings confirm the feasibility of using the PWFS in its unmodulated form\nand highlight its potential for next-generation instruments. Future e fforts will focus on achieving even higher control loop frequencies\n(>3 kHz), optimizing the calibration procedures, and testing its performance on fainter stars, where more gain is expected for the\nunmodulated PWFS compared to its modulated counterpart.\nKey words. instrumentation: adaptive optics \u2013 instrumentation: high angular resolution"
  },
  {
    "filename": "Do_regularization_methods_for_shortcut_mitigation_work_as_intended?_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nDo Regularization Methods for Shortcut Mitigation Work As\nIntended?\nHaoyang Hong Ioanna Papanikolaou Sonali Parbhoo\nImperial College London Imperial College London Imperial College London\n\n=== ABSTRACT ===\n\nMitigating shortcuts, where models exploit\nspurious correlations in training data, re-\nmains a significant challenge for improv-\ning generalization. Regularization methods\nhave been proposed to address this issue\nby enhancing model generalizability. How-\never, we demonstrate that these methods can\nsometimes overregularize, inadvertently sup-\npressing causal features along with spurious\nones. In this work, we analyze the theo-\nretical mechanisms by which regularization\nmitigates shortcuts and explore the limits of\nits effectiveness. Additionally, we identify\nthe conditions under which regularization can\nsuccessfully eliminate shortcuts without com-\npromising causal features. Through experi-\nments on synthetic and real-world datasets,\nour comprehensive analysis provides valuable\ninsights into the strengths and limitations\nof regularization techniques for addressing\nshortcuts, offering guidance for developing\nmore robust models."
  },
  {
    "filename": "Beyond_Accuracy:_What_Matters_in_Designing_Well-Behaved_Models?_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nBeyond Accuracy: What Matters in Designing Well-Behaved Models?\nRobin Hesse1*Do\u02d8gukan Ba \u02d8gc\u01311*Bernt Schiele2\nSimone Schaub-Meyer1,3Stefan Roth1,3\n1Technical University of Darmstadt2Max Planck Institute for Informatics, SIC3hessian.AI*equal contribution\n\n=== ABSTRACT ===\n\nDeep learning has become an essential part of computer\nvision, with deep neural networks (DNNs) excelling in pre-\ndictive performance. However, they often fall short in other\ncritical quality dimensions, such as robustness, calibration,\nor fairness. While existing studies have focused on a subset\nof these quality dimensions, none have explored a more gen-\neral form of \u201cwell-behavedness\u201d of DNNs. With this work,\nwe address this gap by simultaneously studying nine differ-\nent quality dimensions for image classification. Through a\nlarge-scale study, we provide a bird\u2019s-eye view by analyzing\n326 backbone models and how different training paradigms\nand model architectures affect the quality dimensions. We\nreveal various new insights such that (i)vision-language\nmodels exhibit high fairness on ImageNet-1k classification\nand strong robustness against domain changes; (ii)self-\nsupervised learning is an effective training paradigm to im-\nprove almost all considered quality dimensions; and (iii)the\ntraining dataset size is a major driver for most of the qual-\nity dimensions. We conclude our study by introducing the\nQUBA score ( Quality Understanding Beyond Accuracy),\na novel metric that ranks models across multiple dimen-\nsions of quality, enabling tailored recommendations based\non specific user needs.1"
  },
  {
    "filename": "ML-Based_Bidding_Price_Prediction_for_Pay-As-Bid_Ancillary_Services_Markets:_A_Use_Case_in_the_German_Control_Reserve_Market_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nML-B ASED BIDDING PRICE PREDICTION FOR PAY-AS-BID\nANCILLARY SERVICES MARKETS : A U SECASE IN THE GERMAN\nCONTROL RESERVE MARKET\nVincent Bezold1, *\nFraunhofer Institute for Manufacturing Engineering and Automation IPA,\nNobelstra\u00dfe 12, Stuttgart, 70569, Germany\nORCID: 0009-0006-4430-4710\nvincent.bezold@ipa.fraunhofer.de\nLukas Baur1\nFraunhofer Institute for Manufacturing Engineering and Automation IPA,\nNobelstra\u00dfe 12, Stuttgart, 70569, Germany\nORCID: 0000-0002-6265-9877\nlukas.baur@ipa.fraunhofer.de\nAlexander Sauer\nFraunhofer Institute for Manufacturing Engineering and Automation IPA,\nNobelstra\u00dfe 12, Stuttgart, 70569, Germany\nORCID: 0000-0003-3822-1514\nMarch 24, 2025\n1equal contribution\n*corresponding author\n\n=== ABSTRACT ===\n\nThe increasing integration of renewable energy sources has led to greater volatility and unpredictability\nin electricity generation, posing challenges to grid stability. Ancillary service markets, such as the\nGerman control reserve market, allow industrial consumers and producers to offer flexibility in their\npower consumption or generation, contributing to grid stability while earning additional income.\nHowever, many participants use simple bidding strategies that may not maximize their revenues.\nThis paper presents a methodology for forecasting bidding prices in pay-as-bid ancillary service\nmarkets, focusing on the German control reserve market. We evaluate various machine learning\nmodels, including Support Vector Regression, Decision Trees, and k-Nearest Neighbors, and compare\ntheir performance against benchmark models. To address the asymmetry in the revenue function\nof pay-as-bid markets, we introduce an offset adjustment technique that enhances the practical\napplicability of the forecasting models. Our analysis demonstrates that the proposed approach\nimproves potential revenues by 27.43 % to 37.31 % compared to baseline models. When analyzing\nthe relationship between the model forecasting errors and the revenue, a negative correlation is\nmeasured for three markets; according to the results, a reduction of 1 C/MW model price forecasting\nerror (MAE) statistically leads to a yearly revenue increase between 483 C/MW and 3,631 C/MW.\nThe proposed methodology enables industrial participants to optimize their bidding strategies,\nleading to increased earnings and contributing to the efficiency and stability of the electrical grid.arXiv:2503.17214v1  [cs.LG]  21 Mar 2025\nAPREPRINT - M ARCH 24, 2025"
  },
  {
    "filename": "Capturing_Individual_Human_Preferences_with_Reward_Features_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nCapturing Individual Human Preferences with Reward Features\nAndr \u00b4e Barreto1Vincent Dumoulin1Yiran Mao1Nicolas Perez-Nieves1Bobak Shahriari1Yann Dauphin1\nDoina Precup1Hugo Larochelle1\n\n=== ABSTRACT ===\n\nReinforcement learning from human feedback\nusually models preferences using a reward model\nthat does not distinguish between people. We ar-\ngue that this is unlikely to be a good design choice\nin contexts with high potential for disagreement,\nlike in the training of large language models. We\npropose a method to specialise a reward model\nto a person or group of people. Our approach\nbuilds on the observation that individual prefer-\nences can be captured as a linear combination of\na set of general reward features. We show how to\nlearn such features and subsequently use them to\nquickly adapt the reward model to a specific indi-\nvidual, even if their preferences are not reflected\nin the training data. We present experiments with\nlarge language models comparing the proposed\narchitecture with a non-adaptive reward model\nand also adaptive counterparts, including models\nthat do in-context personalisation. Depending on\nhow much disagreement there is in the training\ndata, our model either significantly outperforms\nthe baselines or matches their performance with a\nsimpler architecture and more stable training."
  },
  {
    "filename": "Lie_Detector:_Unified_Backdoor_Detection_via_Cross-Examination_Framework_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nLie Detector: Unified Backdoor Detection via Cross-Examination Framework\nXuan Wang\nwangxuan21d@nudt.edu.cnSiyuan Liang\nsiyuan96@nus.edu.sgDongping Liao\ndongpingliao@umac.mo\nHan Fang\nfanghan@nus.edu.sgAishan Liu\nliuaishan@buaa.edu.cnXiaochun Cao\ncaoxiaochun@mail.sysu.edu.cn\nYuliang Lu\npublicluyl@126.comChang Ee-Chien\nchangec@comp.nus.edu.sgXitong Gao\nxt.gao@siat.ac.cn\n\n=== ABSTRACT ===\n\nInstitutions with limited data and computing resources often\noutsource model training to third-party providers in a semi-\nhonest setting, assuming adherence to prescribed training\nprotocols with pre-defined learning paradigm (e.g., super-\nvised or semi-supervised learning). However, this practice\ncan introduce severe security risks, as adversaries may poi-\nson the training data to embed backdoors into the resulting\nmodel. Existing detection approaches predominantly rely\non statistical analyses, which often fail to maintain univer-\nsally accurate detection accuracy across different learning\nparadigms. To address this challenge, we propose a uni-\nfied backdoor detection framework in the semi-honest set-\nting that exploits cross-examination of model inconsisten-\ncies between two independent service providers. Specifi-\ncally, we integrate central kernel alignment to enable ro-\nbust feature similarity measurements across different model\narchitectures and learning paradigms, thereby facilitating\nprecise recovery and identification of backdoor triggers. We\nfurther introduce backdoor fine-tuning sensitivity analysis\nto distinguish backdoor triggers from adversarial pertur-\nbations, substantially reducing false positives. Extensive\nexperiments demonstrate that our method achieves supe-\nrior detection performance, improving accuracy by 5.4%,\n1.6%, and 11.9% over SoTA baselines across supervised,\nsemi-supervised, and autoregressive learning tasks, respec-\ntively. Notably, it is the first to effectively detect backdoors\nin multimodal large language models, further highlighting\nits broad applicability and advancing secure deep learning."
  },
  {
    "filename": "Calibration_Strategies_for_Robust_Causal_Estimation:_Theoretical_and_Empirical_Insights_on_Propensity_Score_Based_Estimators_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nCalibration Strategies for Robust Causal Estimation:\nTheoretical and Empirical Insights on Propensity Score\nBased Estimators\nJan Rabenseifner1,2,\u2020, Sven Klaassen2,1, Jannis Kueck3, and Philipp Bach1\n1University of Hamburg, Germany\n2Economic AI, Germany\n3Heinrich Heine University D \u00a8usseldorf, D \u00a8usseldorf Institute for Competition Economics, Germany\nMarch 24, 2025\n\n=== ABSTRACT ===\n\nThe partitioning of data for estimation and calibration critically impacts the perfor-\nmance of propensity score based estimators like inverse probability weighting (IPW)\nand double/debiased machine learning (DML) frameworks. We extend recent advances\nin calibration techniques for propensity score estimation, improving the robustness of\npropensity scores in challenging settings such as limited overlap, small sample sizes, or\nunbalanced data. Our contributions are twofold: First, we provide a theoretical anal-\nysis of the properties of calibrated estimators in the context of DML. To this end, we\nrefine existing calibration frameworks for propensity score models, with a particular\nemphasis on the role of sample-splitting schemes in ensuring valid causal inference.\nSecond, through extensive simulations, we show that calibration reduces variance of\ninverse-based propensity score estimators while also mitigating bias in IPW, even in\nsmall-sample regimes. Notably, calibration improves stability for flexible learners (e.g.,\ngradient boosting) while preserving the doubly robust properties of DML. A key insight\nis that, even when methods perform well without calibration, incorporating a calibra-\ntion step does not degrade performance, provided that an appropriate sample-splitting\napproach is chosen."
  },
  {
    "filename": "Limits_of_trust_in_medical_AI_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\n1 \n \nThis is a pre-print version of an article  published as : \nHatherley, Joshua. 2020. Limits of trust in medical AI.  Journal of Medical Ethics  46(7): 478 -\n481. 10.1136/medethics -2019 -105935 . \nPlease cite that version.  \n2 \n \nLIMITS OF TRUST IN MEDICAL  AI \nJoshua Hatherley\n\n=== ABSTRACT ===\n\n: Artificial intelligence (AI) is expected to revolutionise the practice of medicine. \nRecent advancements in the field of deep learning have demonstrated success \nin variety of clinical tasks: detecting diabetic retinopathy from images, predict-\ning hospital re admissions, aiding in the discovery of new drugs, etc. AI\u2019s pro-\ngress in medicine, however, has led to concerns regarding the potential effects \nof this technology upon relationships of trust in clinical practice. In this paper, \nI will argue that there is me rit to these concerns, since AI systems can be relied \nupon, and are capable of reliability, but cannot be trusted, and are not capable \nof trustworthiness. Insofar as patients are required to rely upon AI systems for \ntheir medical decision -making, there is potential for this to produce a deficit of \ntrust in  relationships in clinical practice."
  },
  {
    "filename": "TeMP-TraG:_Edge-based_Temporal_Message_Passing_in_Transaction_Graphs_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nTeMP-TraG: Edge-based Temporal Message\nPassing in Transaction Graphs\nSteve Gounoue1,2( \u0000), Ashutosh Sao3, and Simon Gottschalk3\n1Data Science and Intelligent Systems Group (DSIS), University of Bonn, Bonn,\nGermany steve.gounoue@cs.uni-bonn.de\n2Lamarr Institute for Machine Learning and Artificial Intelligence, Bonn, Germany\n3L3S Research Center, Hannover, Germany {sao,gottschalk}@l3s.de\n\n=== ABSTRACT ===\n\n. Transactiongraphs,whichrepresentfinancialandtradetrans-\nactions between entities such as bank accounts and companies, can re-\nveal patterns indicative of financial crimes like money laundering and\nfraud. However, effective detection of such cases requires node and edge\nclassification methods capable of addressing the unique challenges of\ntransaction graphs, including rich edge features, multigraph structures\nand temporal dynamics. To tackle these challenges, we propose TeMP-\nTraG,anovelgraphneuralnetworkmechanismthatincorporatestempo-\nral dynamics into message passing. TeMP-TraG prioritises more recent\ntransactions when aggregating node messages, enabling better detection\nof time-sensitive patterns. We demonstrate that TeMP-TraG improves\nfour state-of-the-art graph neural networks by 6.19%on average. Our re-\nsults highlight TeMP-TraG as an advancement in leveraging transaction\ngraphs to combat financial crime."
  },
  {
    "filename": "Jailbreaking_the_Non-Transferable_Barrier_via_Test-Time_Data_Disguising_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nJailbreaking the Non-Transferable Barrier via Test-Time Data Disguising\nYongli Xiang1, Ziming Hong1\u2020, Lina Yao2,3, Dadong Wang2, Tongliang Liu1\u2020\n1Sydney AI Centre, The University of Sydney\n2Data61, CSIRO3The University of New South Wales\n\n=== ABSTRACT ===\n\nNon-transferable learning (NTL) has been proposed to pro-\ntect model intellectual property (IP) by creating a \u201cnon-\ntransferable barrier\u201d to restrict generalization from autho-\nrized to unauthorized domains. Recently, well-designed at-\ntack, which restores the unauthorized-domain performance\nby fine-tuning NTL models on few authorized samples, high-\nlights the security risks of NTL-based applications. How-\never, such attack requires modifying model weights, thus\nbeing invalid in the black-box scenario. This raises a crit-\nical question: can we trust the security of NTL models\ndeployed as black-box systems? In this work, we reveal\nthe first loophole of black-box NTL models by proposing\na novel attack method (dubbed as JailNTL) to jailbreak\nthe non-transferable barrier through test-time data disguis-\ning. The main idea of JailNTL is to disguise unautho-\nrized data so it can be identified as authorized by the NTL\nmodel, thereby bypassing the non-transferable barrier with-\nout modifying the NTL model weights. Specifically, JailNTL\nencourages unauthorized-domain disguising in two levels,\nincluding: (i) data-intrinsic disguising (DID) for eliminat-\ning domain discrepancy and preserving class-related con-\ntent at the input-level, and (ii) model-guided disguising\n(MGD) for mitigating output-level statistics difference of\nthe NTL model. Empirically, when attacking state-of-the-\nart (SOTA) NTL models in the black-box scenario, Jail-\nNTL achieves an accuracy increase of up to 55.7% in the\nunauthorized domain by using only 1% authorized samples,\nlargely exceeding existing SOTA white-box attacks. Code is\nreleased at https://github.com/tmllab/2025_\nCVPR_JailNTL ."
  },
  {
    "filename": "NdLinear_Is_All_You_Need_for_Representation_Learning_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nNdLinear Is All You Need for Representation Learning\nAlex Reneau1Jerry Yao-Chieh Hu2Zhongfang Zhuang3Ting-Chun Liu4\nEnsemble AI\n\n=== ABSTRACT ===\n\nMany high-impact machine learning tasks involve multi-dimensional data (e.g., images, vol-\numetric medical scans, multivariate time-series). Yet, most neural architectures flatten inputs,\ndiscarding critical cross-dimension information. We introduce NdLinear, a novel linear trans-\nformation that preserves these structures without extra overhead. By operating separately\nalong each dimension, NdLinear captures dependencies that standard fully connected lay-\ners overlook. Extensive experiments across convolutional, recurrent, and transformer-based\nnetworks show significant improvements in representational power and parameter efficiency.\nCrucially, NdLinear serves as a foundational building block for large-scale foundation models\nby operating on any unimodal or multimodal data in its native form. This removes the need\nfor flattening or modality-specific preprocessing. Ndlinear rethinks core architectural priori-\nties beyond attention, enabling more expressive, context-aware models at scale. We propose\nNdLinear as a drop-in replacement for standard linear layers \u2014 marking an important step\ntoward next-generation neural architectures.\n1alex@ensemblecore.ai\n2jhu@ensemblecore.ai\n3zhongfang@ensemblecore.ai\n4ting-chun@ensemblecore.ai\nCode is available at https://github.com/ensemble-core/NdLineararXiv:2503.17353v1  [cs.LG]  21 Mar 2025\nContents\n1 Introduction 1\n2 NdLinear: N-dimensional Linear Transformation 3\n2.1 NdLinear: N-dimensional Linear Transformation . . . . . . . . . . . . . . . . . 4\n2.2 Complexity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.3 Implementation Details and Training Protocol . . . . . . . . . . . . . . . . . . . 9\n3 NdLinear Is All You Need: Versatile and Seamless Integration with Deep Learning\nArchitectures 11\n3.1 Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3.2 Multi-Layer Perceptrons (MLPs) . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.3 Recurrent Neural Networks (RNNs) . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.4 Convolutional Neural Networks (CNNs) . . . . . . . . . . . . . . . . . . . . . . 14\n4 Preliminary Experimental Results 15\n4.1 NdLinear Layers in Classification / Regression Head . . . . . . . . . . . . . . . 15\n4.2 NdLinear Layers in Feature Extraction Blocks . . . . . . . . . . . . . . . . . . . 20\n4.3 Large Language Models (LLMs) with NdLinear . . . . . . . . . . . . . . . . . . 26\n5 Discussion and Conclusion 27\nA Comparison with Structured Representation Learning Methods 29\nB More Related Work 31"
  },
  {
    "filename": "Efficient_Training_of_Neural_Fractional-Order_Differential_Equation_via_Adjoint_Backpropagation_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nEfficient Training of Neural Fractional-Order Differential Equation\nvia Adjoint Backpropagation\nQiyu Kang1, Xuhao Li\u20202, Kai Zhao3, Wenjun Cui4, Yanan Zhao3, Weihua Deng5, Wee Peng Tay3\n1University of Science and Technology of China\n2Anhui University\n3Nanyang Technological University\n4Beijing Jiaotong University\n5Lanzhou University\n\n=== ABSTRACT ===\n\nFractional-order differential equations (FDEs) enhance tradi-\ntional differential equations by extending the order of differen-\ntial operators from integers to real numbers, offering greater\nflexibility in modeling complex dynamical systems with non-\nlocal characteristics. Recent progress at the intersection of\nFDEs and deep learning has catalyzed a new wave of innova-\ntive models, demonstrating the potential to address challenges\nsuch as graph representation learning. However, training neu-\nral FDEs has primarily relied on direct differentiation through\nforward-pass operations in FDE numerical solvers, leading\nto increased memory usage and computational complexity,\nparticularly in large-scale applications. To address these chal-\nlenges, we propose a scalable adjoint backpropagation method\nfor training neural FDEs by solving an augmented FDE back-\nward in time, which substantially reduces memory require-\nments. This approach provides a practical neural FDE toolbox\nand holds considerable promise for diverse applications. We\ndemonstrate the effectiveness of our method in several tasks,\nachieving performance comparable to baseline models while\nsignificantly reducing computational overhead.\nCode \u2014 https://github.com/kangqiyu/torchfde"
  },
  {
    "filename": "Learning_Part_Knowledge_to_Facilitate_Category_Understanding_for_Fine-Grained_Generalized_Category_Discovery_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nLearning Part Knowledge to Facilitate Category Understanding for\nFine-Grained Generalized Category Discovery\nEnguang Wang1, Zhimao Peng1, Zhengyuan Xie1, Haori Lu1, Fei Yang2,1, Xialei Liu2,1\n1VCIP, CS, Nankai University2NKIARI, Shenzhen Futian\n{enguangwang,zhimao796,xiezhengyuan,luhaori }@mail.nankai.edu.cn\n{feiyang,xialei }@nankai.edu.cn\n\n=== ABSTRACT ===\n\nGeneralized Category Discovery (GCD) aims to clas-\nsify unlabeled data containing both seen and novel cate-\ngories. Although existing methods perform well on generic\ndatasets, they struggle in fine-grained scenarios. We at-\ntribute this difficulty to their reliance on contrastive learn-\ning over global image features to automatically capture dis-\ncriminative cues, which fails to capture the subtle local\ndifferences essential for distinguishing fine-grained cate-\ngories. Therefore, in this paper, we propose incorporating\npart knowledge to address fine-grained GCD, which intro-\nduces two key challenges: the absence of annotations for\nnovel classes complicates the extraction of the part fea-\ntures, and global contrastive learning prioritizes holistic\nfeature invariance, inadvertently suppressing discrimina-\ntive local part patterns. To address these challenges, we\npropose PartGCD , including 1) Adaptive Part Decompo-\nsition, which automatically extracts class-specific seman-\ntic parts via Gaussian Mixture Models, and 2) Part Dis-\ncrepancy Regularization, enforcing explicit separation be-\ntween part features to amplify fine-grained local part dis-\ntinctions. Experiments demonstrate state-of-the-art per-\nformance across multiple fine-grained benchmarks while\nmaintaining competitiveness on generic datasets, validat-\ning the effectiveness and robustness of our approach."
  },
  {
    "filename": "Principal_Eigenvalue_Regularization_for_Improved_Worst-Class_Certified_Robustness_of_Smoothed_Classifiers_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nPrincipal Eigenvalue Regularization for Improved Worst-Class Certified\nRobustness of Smoothed Classifiers\nGaojie Jin1Tianjin Huang1Ronghui Mu1Xiaowei Huang2\n\n=== ABSTRACT ===\n\nRecent studies have identified a critical challenge\nin deep neural networks (DNNs) known as \u201crobust\nfairness\u201d, where models exhibit significant dispar-\nities in robust accuracy across different classes.\nWhile prior work has attempted to address this\nissue in adversarial robustness, the study of worst-\nclass certified robustness for smoothed classifiers\nremains unexplored. Our work bridges this gap by\ndeveloping a PAC-Bayesian bound for the worst-\nclass error of smoothed classifiers. Through the-\noretical analysis, we demonstrate that the largest\neigenvalue of the smoothed confusion matrix fun-\ndamentally influences the worst-class error of\nsmoothed classifiers. Based on this insight, we in-\ntroduce a regularization method that optimizes the\nlargest eigenvalue of smoothed confusion matrix\nto enhance worst-class accuracy of the smoothed\nclassifier and further improve its worst-class certi-\nfied robustness. We provide extensive experimen-\ntal validation across multiple datasets and model\narchitectures to demonstrate the effectiveness of\nour approach."
  },
  {
    "filename": "Rethinking_the_Role_of_Spatial_Mixing_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nRethinking the Role of Spatial Mixing\nGeorge Cazenavette1* Joel Julin2Simon Lucey3\n1Massachusetts Institute of Technology2Carnegie Mellon University3University of Adelaide\n\n=== ABSTRACT ===\n\nUntil quite recently, the backbone of nearly every state-of-\nthe-art computer vision model has been the 2D convolution.\nAt its core, a 2D convolution simultaneously mixes informa-\ntion across both the spatial and channel dimensions of a\nrepresentation. Many recent computer vision architectures\nconsist of sequences of isotropic blocks that disentangle the\nspatial and channel-mixing components. This separation of\nthe operations allows us to more closely juxtapose the effects\nof spatial and channel mixing in deep learning. In this paper,\nwe take an initial step towards garnering a deeper under-\nstanding of the roles of these mixing operations. Through\nour experiments and analysis, we discover that on both clas-\nsical (ResNet) and cutting-edge (ConvMixer) models, we can\nreach nearly the same level of classification performance by\nonly learning channel mixing and leaving the spatial mixers\nat their random initializations. Furthermore, we show that\nmodels with random, fixed spatial mixing are naturally more\nrobust to adversarial perturbations. Lastly, we show that this\nphenomenon extends past the classification regime, as such\nmodels can also decode pixel-shuffled images."
  },
  {
    "filename": "Token_Dynamics:_Towards_Efficient_and_Dynamic_Video_Token_Representation_for_Video_Large_Language_Models_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nToken Dynamics: Towards Efficient and Dynamic Video Token Representation\nfor Video Large Language Models\nHaichao Zhang\nNortheastern University\n360 Hungtington Ave, Boston MA 02115\nzhang.haichao@northeastern.eduZhuowei Li\nRutgers University\n57 US Highway 1, New Brunswick, NJ 08901\nzl502@cs.rutgers.edu\nDimitris Metaxas\nRutgers University\n57 US Highway 1, New Brunswick, NJ 08901\ndnm@cs.rutgers.eduYun Fu\nNortheastern University\n360 Hungtington Ave, Boston MA 02115\nyunfu@ece.neu.edu\n\n=== ABSTRACT ===\n\nToken-based video representation has emerged as a\npromising approach for enabling large language models\n(LLMs) to interpret video content. However, existing to-\nken reduction techniques, such as token pruning and token\nmerging, often disrupt essential spatial-temporal positional\nembeddings, failing to adequately balance computational\nefficiency with fewer tokens. Consequently, these methods\nresult in relatively lengthy token sequences, limiting their\napplicability in scenarios requiring extreme token compres-\nsion, such as video large language models. In this paper, we\nintroduce the novel task of extreme short token reduction,\naiming to represent extensive video sequences with a min-\nimal number of tokens. To address this challenge, we pro-\npose Token Dynamics , a new video representation frame-\nwork that dynamically reduces token count while preserving\nspatial-temporal coherence. Specifically, we disentangle\nvideo representations by separating visual embeddings from\ngrid-level motion information, structuring them into: (i) a\nconcise token base, created by clustering tokens that de-\nscribe object-level content, and (ii) a token dynamics map,\ncapturing detailed spatial-temporal motion patterns across\ngrids. Furthermore, we introduce a cross-dynamics atten-\ntion mechanism that integrates motion features into the to-\nken base without increasing token length, thereby maintain-\ning compactness and spatial-temporal integrity. We evalu-\nate Token Dynamics on the proposed extreme short token\nreduction task, demonstrating a reduction of token count\nto merely 0.07% of the original tokens, with only a minor\nperformance drop of 1.13% on the NextQA-MC benchmark.\nOur approach similarly maintains competitive performance\non ActNet-QA, Long Video Bench, and VideoMME datasetsin a plug-and-play manner. Additionally, we propose two\nnovel subtasks within extreme token reduction (fixed-length\nand adaptive-length compression) both effectively repre-\nsenting long token sequences for video-language tasks. Our\nmethod achieves state-of-the-art performance across these\nsubtasks, offering significantly lower theoretical complex-\nity, fewer tokens, and enhanced throughput, thus providing\nan efficient solution for video large language models."
  },
  {
    "filename": "Modifying_Large_Language_Model_Post-Training_for_Diverse_Creative_Writing_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nPreprint. Under review.\nModifying Large Language Model Post-Training for Diverse\nCreative Writing\nJohn Joon Young Chung1\u2217, Vishakh Padmakumar2, Melissa Roemmele1,\nYuqian Sun1& Max Kreminski1\n1Midjourney\n2New York University\n\n=== ABSTRACT ===\n\nAs creative writing tasks do not have singular correct answers, large lan-\nguage models (LLMs) trained to perform these tasks should be able to\ngenerate diverse valid writings. However, LLM post-training often focuses\non improving generation quality but neglects to facilitate output diver-\nsity. Hence, in creative writing generation, we investigate post-training\napproaches to promote both output diversity and quality. Our core idea is\nto include deviation\u2014the degree of difference between a training sample\nand all other samples with the same prompt\u2014in the training objective to fa-\ncilitate learning from rare high-quality instances. By adopting our approach\nto direct preference optimization (DPO) and odds ratio preference optimiza-\ntion (ORPO), we demonstrate that we can promote the output diversity\nof trained models while minimally decreasing quality. Our best model\nwith 8B parameters could achieve on-par diversity as a human-created\ndataset while having output quality similar to the best instruction-tuned\nmodels we examined, GPT-4o and DeepSeek-R1. We further validate our\napproaches with a human evaluation, an ablation, and a comparison to an\nexisting diversification approach, DivPO.1"
  },
  {
    "filename": "Imagine_to_Hear:_Auditory_Knowledge_Generation_can_be_an_Effective_Assistant_for_Language_Models_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nImagine to Hear: Auditory Knowledge Generation can be an Effective\nAssistant for Language Models\nSuho Yoo1*Hyunjong Ok1,2\u2217Jaeho Lee2\u2020\n1HJ AILAB2POSTECH\n{uso7d0, hyunjong.ok}@gmail.com, jaeho.lee@postech.ac.kr\n\n=== ABSTRACT ===\n\nLanguage models pretrained on text-only cor-\npora often struggle with tasks that require audi-\ntory commonsense knowledge. Previous work\naddresses this problem by augmenting the lan-\nguage model to retrieve knowledge from ex-\nternal audio databases. This approach has sev-\neral limitations, such as the potential lack of\nrelevant audio in databases and the high costs\nassociated with constructing and querying the\ndatabases. To address these issues, we pro-\npose Imagine to Hear, a novel approach that dy-\nnamically generates auditory knowledge using\ngenerative models. Our framework detects mul-\ntiple audio-related textual spans from the given\nprompt and generates corresponding auditory\nknowledge. We develop several mechanisms\nto efficiently process multiple auditory knowl-\nedge, including a CLAP-based rejection sam-\npler and a language-audio fusion module. Our\nexperiments show that our method achieves\nstate-of-the-art performance on AuditoryBench\nwithout relying on external databases, high-\nlighting the effectiveness of our generation-\nbased approach."
  },
  {
    "filename": "Gumbel-Softmax_Flow_Matching_with_Straight-Through_Guidance_for_Controllable_Biological_Sequence_Generation_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nGumbel-Softmax Flow Matching\nwith Straight-Through Guidance for Controllable\nBiological Sequence Generation\nSophia Tang1,2, Yinuo Zhang1,3, Alexander Tong4,5, Pranam Chatterjee1,6,7,\u2020\n1Department of Biomedical Engineering, Duke University\n2Management and Technology Program, University of Pennsylvania\n3Center of Computational Biology, Duke-NUS Medical School\n4Mila, Quebec AI Institute,5Universit\u00e9 de Montr\u00e9al\n6Department of Computer Science, Duke University\n7Department of Biostatistics and Bioinformatics, Duke University\n\u2020Corresponding author: pranam.chatterjee@duke.edu\n\n=== ABSTRACT ===\n\nFlow matching in the continuous simplex has emerged as a promising strategy\nfor DNA sequence design, but struggles to scale to higher simplex dimensions\nrequired for peptide and protein generation. We introduce Gumbel-Softmax\nFlow and Score Matching , a generative framework on the simplex based on a\nnovel Gumbel-Softmax interpolant with a time-dependent temperature. Using\nthis interpolant, we introduce Gumbel-Softmax Flow Matching by deriving a\nparameterized velocity field that transports from smooth categorical distributions to\ndistributions concentrated at a single vertex of the simplex. We alternatively present\nGumbel-Softmax Score Matching which learns to regress the gradient of the\nprobability density. Our framework enables high-quality, diverse generation and\nscales efficiently to higher-dimensional simplices. To enable training-free guidance,\nwe propose Straight-Through Guided Flows (STGFlow) , a classifier-based guid-\nance method that leverages straight-through estimators to steer the unconditional\nvelocity field toward optimal vertices of the simplex. STGFlow enables efficient\ninference-time guidance using classifiers pre-trained on clean sequences, and\ncan be used with any discrete flow method. Together, these components form a\nrobust framework for controllable de novo sequence generation. We demonstrate\nstate-of-the-art performance in conditional DNA promoter design, sequence-only\nprotein generation, and target-binding peptide design for rare disease treatment."
  },
  {
    "filename": "A_Flexible_Fairness_Framework_with_Surrogate_Loss_Reweighting_for_Addressing_Sociodemographic_Disparities_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nA Flexible Fairness Framework with Surrogate Loss Reweighting for Addressing\nSociodemographic Disparities\nWen Xu1\u2217,Elham Dolatabadi2\n1University of Toronto\n2York University, Vector Institute\nrealwen.xu@mail.utoronto.ca, edolatab@yorku.ca\n\n=== ABSTRACT ===\n\nThis paper presents a new algorithmic fairness\nframework called \u03b1-\u03b2Fair Machine Learning ( \u03b1-\n\u03b2FML), designed to optimize fairness levels across\nsociodemographic attributes. Our framework em-\nploys a new family of surrogate loss functions,\npaired with loss reweighting techniques, allowing\nprecise control over fairness-accuracy trade-offs\nthrough tunable hyperparameters \u03b1and\u03b2. To ef-\nficiently solve the learning objective, we propose\nParallel Stochastic Gradient Descent with Surro-\ngate Loss (P-SGD-S) and establish convergence\nguarantees for both convex and nonconvex loss\nfunctions. Experimental results demonstrate that\nour framework improves overall accuracy while re-\nducing fairness violations, offering a smooth trade-\noff between standard empirical risk minimization\nand strict minimax fairness. Results across multi-\nple datasets confirm its adaptability, ensuring fair-\nness improvements without excessive performance\ndegradation."
  },
  {
    "filename": "On_Privately_Estimating_a_Single_Parameter_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nOn Privately Estimating a Single Parameter\nHilal Asi1John C. Duchi2Kunal Talwar1\n1Apple2Stanford University\nMarch 24, 2025\n\n=== ABSTRACT ===\n\nWe investigate differentially private estimators for individual parameters within larger\nparametric models. While generic private estimators exist, the estimators we provide re-\npose on new local notions of estimand stability, and these notions allow procedures that\nprovide private certificates of their own stability. By leveraging these private certificates,\nwe provide computationally and statistical efficient mechanisms that release private statis-\ntics that are, at least asymptotically in the sample size, essentially unimprovable: they\nachieve instance optimal bounds. Additionally, we investigate the practicality of the al-\ngorithms both in simulated data and in real-world data from the American Community\nSurvey and US Census, highlighting scenarios in which the new procedures are successful\nand identifying areas for future work."
  },
  {
    "filename": "Neuro-Symbolic_Scene_Graph_Conditioning_for_Synthetic_Image_Dataset_Generation_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nNEURO -SYMBOLIC SCENE GRAPH CONDITIONING FOR\nSYNTHETIC IMAGE DATASET GENERATION\nGiacomo Savazzi, Eugenio Lomurno, Cristian Sbrolli, Agnese Chiatti, Matteo Matteucci\nPolitecnico di Milano\nDepartment of Electronics, Information and Bioengineering\nVia Ponzio 34/5, 20133 Milan, Italy\n{giacomo.savazzi}@mail.polimi.it\n{eugenio.lomurno, cristian.sbrolli}@polimi.it\n{agnese.chiatti, matteo.matteucci}@polimi.it\n\n=== ABSTRACT ===\n\nAs machine learning models increase in scale and complexity, obtaining sufficient training data\nhas become a critical bottleneck due to acquisition costs, privacy constraints, and data scarcity in\nspecialised domains. While synthetic data generation has emerged as a promising alternative, a notable\nperformance gap remains compared to models trained on real data, particularly as task complexity\ngrows. Concurrently, Neuro-Symbolic methods, which combine neural networks\u2019 learning strengths\nwith symbolic reasoning\u2019s structured representations, have demonstrated significant potential across\nvarious cognitive tasks. This paper explores the utility of Neuro-Symbolic conditioning for synthetic\nimage dataset generation, focusing specifically on improving the performance of Scene Graph\nGeneration models. The research investigates whether structured symbolic representations in the form\nof scene graphs can enhance synthetic data quality through explicit encoding of relational constraints.\nThe results demonstrate that Neuro-Symbolic conditioning yields significant improvements of up to\n+2.59% in standard Recall metrics and +2.83% in No Graph Constraint Recall metrics when used\nfor dataset augmentation. These findings establish that merging Neuro-Symbolic and generative\napproaches produces synthetic data with complementary structural information that enhances model\nperformance when combined with real data, providing a novel approach to overcome data scarcity\nlimitations even for complex visual reasoning tasks.\nKeywords Synthetic Data Generation \u00b7Neuro-Symbolic AI \u00b7Scene Graph Generation \u00b7Data Augmentation \u00b7Visual\nReasoning."
  },
  {
    "filename": "BEAC:_Imitating_Complex_Exploration_and_Task-oriented_Behaviors_for_Invisible_Object_Nonprehensile_Manipulation_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nBEAC: Imitating Complex Exploration and Task-oriented\nBehaviors for Invisible Object Nonprehensile Manipulation\nHirotaka Taharaa,b,\u2217, Takamitsu Matsubaraa\naNara Institute of Science and Technology, 630-0192, Nara, Japan\nbKobe City College of Technology, 651-2194, Hyogo, Japan\n\n=== ABSTRACT ===\n\nApplying imitation learning (IL) is challenging to nonprehensile manipulation tasks\nof invisible objects with partial observations, such as excavating buried rocks. The\ndemonstrator must make such complex action decisions as exploring to find the ob-\nject and task-oriented actions to complete the task while estimating its hidden state,\nperhaps causing inconsistent action demonstration and high cognitive load problems.\nFor these problems, work in human cognitive science suggests that promoting the use\nof pre-designed, simple exploration rules for the demonstrator may alleviate the prob-\nlems of action inconsistency and high cognitive load. Therefore, when performing\nimitation learning from demonstrations using such exploration rules, it is important to\naccurately imitate not only the demonstrator\u2019s task-oriented behavior but also his/her\nmode-switching behavior (exploratory or task-oriented behavior) under partial observa-\ntion. Based on the above considerations, this paper proposes a novel imitation learning\nframework called Belief Exploration-Action Cloning (BEAC), which has a switching\npolicy structure between a pre-designed exploration policy and a task-oriented action\npolicy trained on the estimated belief states based on past history. In simulation and\nreal robot experiments, we confirmed that our proposed method achieved the best task\nperformance, higher mode and action prediction accuracies, while reducing the cogni-\ntive load in the demonstration indicated by a user study."
  },
  {
    "filename": "TreeSynth:_Synthesizing_Diverse_Data_from_Scratch_via_Tree-Guided_Subspace_Partitioning_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nTREESYNTH : Synthesizing Diverse Data from Scratch\nvia Tree-Guided Subspace Partitioning\nSheng Wang\u2217, Pengan Chen\u2217, Jingqi Zhou\u2217, Qintong Li\nThe University of Hong Kong\n{u3009618, cpa2001, u3011211, qtli}@connect.hku.hkJingwei Dong\nXi\u2019an Jiaotong University\ndongjingwei@stu.xjtu.edu.cn\nJiahui Gao\nThe University of Hong Kong\nggaojiahui@gmail.comBoyang Xue, Jiyue Jiang\nThe Chinese University of Hong Kong\nbyxue@se.cuhk.edu.hk, jiangjy@link.cuhk.edu.hk\nLingpeng Kong, Chuan Wu\nThe University of Hong Kong\n{lpk, cwu}@cs.hku.hk\n\n=== ABSTRACT ===\n\nModel customization requires high-quality and diverse datasets, but acquiring such\ndata remains challenging and costly. Although large language models (LLMs)\ncan synthesize training data, current approaches are constrained by limited seed\ndata, model bias and insufficient control over the generation process, resulting\nin limited diversity and biased distribution with the increase of data scales. To\ntackle this challenge, we present TREESYNTH , a tree-guided subspace-based data\nsynthesis framework that recursively partitions the entire data space into hierar-\nchical subspaces, enabling comprehensive and diverse scaling of data synthesis.\nBriefly, given a task-specific description, we construct a data space partitioning\ntree by iteratively executing criteria determination and subspace coverage steps.\nThis hierarchically divides the whole space ( i.e., root node) into mutually exclusive\nand complementary atomic subspaces ( i.e., leaf nodes). By collecting synthesized\ndata according to the attributes of each leaf node, we obtain a diverse dataset that\nfully covers the data space. Empirically, our extensive experiments demonstrate\nthatTREESYNTH surpasses both human-designed datasets and the state-of-the-art\ndata synthesis baselines, achieving maximum improvements of 45.2% in data\ndiversity and 17.6% in downstream task performance across various models and\ntasks. Hopefully, TREESYNTH provides a scalable solution to synthesize diverse\nand comprehensive datasets from scratch without human intervention."
  },
  {
    "filename": "LoGoFair:_Post-Processing_for_Local_and_Global_Fairness_in_Federated_Learning_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nLoGoFair: Post-Processing for Local and Global Fairness in Federated Learning\nLi Zhang, Chaochao Chen*, Zhongxuan Han, Qiyong Zhong, Xiaolin Zheng\nZhejiang University\nzhanglizl80@gmail.com, {zjuccc, zxhan, youngzhong, xlzheng }@zju.edu.cn\n\n=== ABSTRACT ===\n\nFederated learning (FL) has garnered considerable interest\nfor its capability to learn from decentralized data sources.\nGiven the increasing application of FL in decision-making\nscenarios, addressing fairness issues across different sensi-\ntive groups (e.g., female, male) in FL is crucial. Current re-\nsearch often focuses on facilitating fairness at each client\u2019s\ndata ( local fairness ) or within the entire dataset across all\nclients ( global fairness ). However, existing approaches that\nfocus exclusively on either local or global fairness fail to ad-\ndress two key challenges: ( CH1 )Under statistical hetero-\ngeneity, global fairness does not imply local fairness, and\nvice versa. (CH2 )Achieving fairness under model-agnostic\nsetting. To tackle the aforementioned challenges, this paper\nproposes a novel post-processing framework for achieving\nboth Local and GlobalFairness in the FL context, namely\nLoGoFair. To address CH1 , LoGoFair endeavors to seek the\nBayes optimal classifier under local and global fairness con-\nstraints, which strikes the optimal accuracy-fairness balance\nin the probabilistic sense. To address CH2 , LoGoFair em-\nploys a model-agnostic federated post-processing procedure\nthat enables clients to collaboratively optimize global fair-\nness while ensuring local fairness, thereby achieving the op-\ntimal fair classifier within FL. Experimental results on three\nreal-world datasets further illustrate the effectiveness of the\nproposed LoGoFair framework. Code is available at https:\n//github.com/liizhang/LoGofair."
  },
  {
    "filename": "A_preliminary_data_fusion_study_to_assess_the_feasibility_of_Foundation_Process-Property_Models_in_Laser_Powder_Bed_Fusion_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nA preliminary data fusion study to assess the feasibility of\nFoundation Process-Property Models in Laser Powder Bed Fusion\nOriol Vendrell-Gallart1, Nima Negarandeh1, Zahra Zanjani Foumani1, Mahsa Amiri2, Lorenzo\nValdevit\u20202,3, and Ramin Bostanabad\u20201,4\n1Department of Mechanical and Aerospace Engineering, University of California, Irvine, CA,\n92697, USA\n2Materials and Manufacturing Technology Program, University of California, Irvine, CA, 92697,\nUSA\n3Department of Materials Science and Engineering, University of California, Irvine, CA, 92697,\nUSA\n4Department of Civil and Environmental Engineering, University of California, Irvine, CA,\n92697, USA\n\n=== ABSTRACT ===\n\nFoundation models are at the forefront of an increasing number of critical applications. In regards to\ntechnologies such as additive manufacturing (AM), these models have the potential to dramatically acceler-\nate process optimization and, in turn, design of next generation materials. A major challenge that impedes\nthe construction of foundation process-property models is data scarcity. To understand the impact of this\nchallenge, and since foundation models rely on data fusion, in this work we conduct controlled experiments\nwhere we focus on the transferability of information across different material systems and properties. More\nspecifically, we generate experimental datasets from 17-4 PH and 316L stainless steels (SSs) in Laser Pow-\nder Bed Fusion (LPBF) where we measure the effect of five process parameters on porosity and hardness.\nWe then leverage Gaussian processes (GPs) for process-property modeling in various configurations to test\nif knowledge about one material system or property can be leveraged to build more accurate machine learn-\ning models for other material systems or properties. Through extensive cross-validation studies and probing\nthe GPs\u2019 interpretable hyperparameters, we study the intricate relation among data size and dimensional-\nity, complexity of the process-property relations, noise, and characteristics of machine learning models.\nOur findings highlight the need for structured learning approaches that incorporate domain knowledge in\nbuilding foundation process-property models rather than relying on uninformed data fusion in data-limited\napplications."
  },
  {
    "filename": "Ordered_Topological_Deep_Learning:_a_Network_Modeling_Case_Study_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nOrdered Topological Deep Learning: a Network Modeling Case\nStudy\nGuillermo Bern\u00e1rdez\u22171, Miquel Ferriol-Galm\u00e9s\u22172, Carlos G\u00fcemes-Palau2, Mathilde Papillon1,\nPere Barlet-Ros2, Albert Cabellos-Aparicio2, Nina Miolane1\n1UC Santa Barbara, USA2Universitat Politecnica de Catalunya, Spain\n\n=== ABSTRACT ===\n\nComputer networks are the foundation of modern digital infras-\ntructure, facilitating global communication and data exchange. As\ndemand for reliable high-bandwidth connectivity grows, advanced\nnetwork modeling techniques become increasingly essential to\noptimize performance and predict network behavior. Traditional\nmodeling methods, such as packet-level simulators and queueing\ntheory, have notable limitations \u2013either being computationally\nexpensive or relying on restrictive assumptions that reduce accu-\nracy. In this context, the deep learning-based RouteNet family of\nmodels has recently redefined network modeling by showing an\nunprecedented cost-performance trade-off. In this work, we revisit\nRouteNet\u2019s sophisticated design and uncover its hidden connec-\ntion to Topological Deep Learning (TDL), an emerging field that\nmodels higher-order interactions beyond standard graph-based\nmethods. We demonstrate that, although originally formulated as\na heterogeneous Graph Neural Network, RouteNet serves as the\nfirst instantiation of a new form of TDL. More specifically, this\npaper presents OrdGCCN, a novel TDL framework that introduces\nthe notion of ordered neighbors in arbitrary discrete topological\nspaces, and shows that RouteNet\u2019s architecture can be naturally\ndescribed as an ordered topological neural network. To the best of\nour knowledge, this marks the first successful real-world applica-\ntion of state-of-the-art TDL principles \u2013which we confirm through\nextensive testbed experiments\u2013, laying the foundation for the next\ngeneration of ordered TDL-driven applications.\nKeywords\nTopological Deep Learning, Graph Neural Networks, Computer\nNetwork Modeling"
  },
  {
    "filename": "Large_Language_Model_Compression_via_the_Nested_Activation-Aware_Decomposition_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nLarge Language Model Compression via the\nNested Activation-Aware Decomposition\nJun Lu\u2217, Tianyi Xu, Bill Ding, David Li, Yu Kang\u2020\n\n=== ABSTRACT ===\n\nIn this paper, we tackle the critical challenge of compressing large language mod-\nels (LLMs) to facilitate their practical deployment and broader adoption. We in-\ntroduce a novel post-training compression paradigm that focuses on low-rank de-\ncomposition of LLM weights. Our analysis identifies two main challenges in this\ntask: the variability in LLM activation distributions and handling unseen activa-\ntions from different datasets and models.\nTo address these challenges, we propose a nested activation-aware framework\n(NSVD) for LLMs, a training-free approach designed to enhance the accuracy of\nlow-rank decompositions by managing activation outliers through transforming\nthe weight matrix based on activation distribution and the original weight matrix.\nThis method allows for the absorption of outliers into the transformed weight ma-\ntrix, improving decomposition accuracy. Our comprehensive evaluation across\neight datasets and six models from three distinct LLM families demonstrates the\nsuperiority of NSVD over current state-of-the-art methods, especially at medium\nto large compression ratios or in multilingual and multitask settings."
  },
  {
    "filename": "Optimal_Nonlinear_Online_Learning_under_Sequential_Price_Competition_via_s-Concavity_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nOptimal Nonlinear Online Learning under Sequential Price\nCompetition via s-Concavity\nDaniele Bracale dbracale@umich.edu\nDepartment of Statistics\nUniversity of Michigan\nMoulinath Banerjee moulib@umich.edu\nDepartment of Statistics\nUniversity of Michigan\nCong Shi congshi@bus.miami.edu\nUniversity of Miami\nDepartment of Management\nYuekai Sun yuekai@umich.edu\nDepartment of Statistics\nUniversity of Michigan\n\n=== ABSTRACT ===\n\nWe consider price competition among multiple sellers over a selling horizon of Tperiods. In\neach period, sellers simultaneously offer their prices and subsequently observe their respective\ndemand that is unobservable to competitors. The demand function for each seller depends\non all sellers\u2019 prices through a private, unknown, and nonlinear relationship. To address\nthis challenge, we propose a semi-parametric least-squares estimation of the nonlinear mean\nfunction, which does not require sellers to communicate demand information. We show that\nwhen all sellers employ our policy, their prices converge at a rate of O(T\u22121/7)to the Nash\nequilibrium prices that sellers would reach if they were fully informed. Each seller incurs a\nregret ofO(T5/7)relative to a dynamic benchmark policy. A theoretical contribution of our\nwork is proving the existence of equilibrium under shape-constrained demand functions via\nthe concept of s-concavity and establishing regret bounds of our proposed policy. Technically,\nwe also establish new concentration results for the least squares estimator under shape\nconstraints. Our findings offer significant insights into dynamic competition-aware pricing\nand contribute to the broader study of non-parametric learning in strategic decision-making."
  },
  {
    "filename": "NeuroSep-CP-LCB:_A_Deep_Learning-based_Contextual_Multi-armed_Bandit_Algorithm_with_Uncertainty_Quantification_for_Early_Sepsis_Prediction_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nNEURO SEP-CP-LCB: A D EEPLEARNING -BASED CONTEXTUAL\nMULTI -ARMED BANDIT ALGORITHM WITH UNCERTAINTY\nQUANTIFICATION FOR EARLY SEPSIS PREDICTION\nAnni Zhou, Raheem Beyah\nSchool of Electrical and Computer Engineering\nGeorgia Institute of Technology\n{azhou60, ab207}@gatech.eduRishikesan Kamaleswaran\nSchool of Medicine, Department of Surgery\nDuke University\nr.kamaleswaran@duke.edu\n\n=== ABSTRACT ===\n\nIn critical care settings, timely and accurate predictions can significantly impact patient outcomes,\nespecially for conditions like sepsis, where early intervention is crucial. We aim to model patient-\nspecific reward functions in a contextual multi-armed bandit setting. The goal is to leverage patient-\nspecific clinical features to optimize decision-making under uncertainty. In this paper, we propose\nNeuroSep-CP-LCB, a novel integration of neural networks with contextual bandits and conformal\nprediction tailored for early sepsis detection. Unlike the algorithm pool selection problem in the\nprevious paper, where the primary focus was identifying the most suitable pre-trained model for\nprediction tasks, this work directly models the reward function using a neural network, allowing\nfor personalized and adaptive decision-making. Combining the representational power of neural\nnetworks with the robustness of conformal prediction intervals, this framework explicitly accounts\nfor uncertainty in offline data distributions and provides actionable confidence bounds on predictions.\nKeywords Reinforcement learning \u00b7Sepsis \u00b7Early prediction \u00b7Conformal prediction \u00b7Uncertainty quantification"
  },
  {
    "filename": "Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event_Cameras_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nUnsupervised Joint Learning of Optical Flow and Intensity with Event Cameras\nShuang Guo1, Friedhelm Hamann1,3and Guillermo Gallego1,2,3.\n1TU Berlin and Robotics Institute Germany,\n2Einstein Center for Digital Future,3Science of Intelligence Excellence Cluster.\n\n=== ABSTRACT ===\n\nEvent cameras rely on motion to obtain information about\nscene appearance. In other words, for event cameras, mo-\ntion and appearance are seen both or neither, which are\nencoded in the output event stream. Previous works con-\nsider recovering these two visual quantities as separate\ntasks, which does not fit with the nature of event cameras\nand neglects the inherent relations between both tasks. In\nthis paper, we propose an unsupervised learning framework\nthat jointly estimates optical flow (motion) and image inten-\nsity (appearance), with a single network. Starting from the\nevent generation model, we newly derive the event-based\nphotometric error as a function of optical flow and image\nintensity, which is further combined with the contrast maxi-\nmization framework, yielding a comprehensive loss function\nthat provides proper constraints for both flow and intensity\nestimation. Exhaustive experiments show that our model\nachieves state-of-the-art performance for both optical flow\n(achieves 20% and 25% improvement in EPE and AE re-\nspectively in the unsupervised learning category) and in-\ntensity estimation (produces competitive results with other\nbaselines, particularly in high dynamic range scenarios).\nLast but not least, our model achieves shorter inference time\nthan all the other optical flow models and many of the image\nreconstruction models, while they output only one quantity.\nProject page: https://github.com/tub-rip/e2fai"
  },
  {
    "filename": "3D_Neural_Operator-Based_Flow_Surrogates_around_3D_geometries:_Signed_Distance_Functions_and_Derivative_Constraints_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\n3D Neural Operator-Based Flow Surrogates around 3D geometries:\nSigned Distance Functions and Derivative Constraints\nAli Rabeh, Adarsh Krishnamurthy, Baskar Ganapathysubramanian*\n\n=== ABSTRACT ===\n\nAccurate modeling of fluid dynamics around complex geometries is critical for applications such as aerodynamic\noptimization and biomedical device design. While advancements in numerical methods and high-performance com-\nputing have improved simulation capabilities, the computational cost of high-fidelity 3D flow simulations remains\na significant challenge. Scientific machine learning (SciML) offers an efficient alternative, enabling rapid and reli-\nable flow predictions. In this study, we evaluate Deep Operator Networks (DeepONet) and Geometric-DeepONet, a\nvariant that incorporates geometry information via signed distance functions (SDFs), on steady-state 3D flow over\ncomplex objects. Our dataset consists of 1,000 high-fidelity simulations spanning Reynolds numbers from 10 to\n1,000, enabling comprehensive training and evaluation across a range of flow regimes. To assess model generaliza-\ntion, we apply two train\u2013test splitting strategies: (1) a random split, evaluating (interpolation) performance within\nthe data distribution, and (2) an extrapolatory split, testing performance on unseen flow conditions. Additionally, we\nexplore a derivative-informed training strategy that augments standard loss functions with velocity gradient penal-\nties and incompressibility constraints, improving physics consistency in 3D flow prediction. Our results show that\nGeometric-DeepONet improves boundary-layer accuracy by up to 32% compared to standard DeepONet. Moreover,\nincorporating derivative constraints enhances gradient accuracy by 25% in interpolation tasks and up to 45% in ex-\ntrapolatory test scenarios, suggesting significant improvement in generalization capabilities to unseen 3D Reynolds\nnumbers. By refining loss function design and integrating rigorous evaluation metrics, this study advances SciML\nmethodologies for scalable and generalizable high-fidelity 3D fluid flow modeling around complex geometries."
  },
  {
    "filename": "Preference-Guided_Diffusion_for_Multi-Objective_Offline_Optimization_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nPreference-Guided Diffusion for Multi-Objective Offline\nOptimization\nYashas Annadani1,3, Syrine Belakaria2, Stefano Ermon2, Stefan Bauer1,3, and Barbara E.\nEngelhardt2, 4\n1Technical University of Munich2Stanford University3Helmholtz AI, Munich\n4Gladstone Institutes\n\n=== ABSTRACT ===\n\nOffline multi-objective optimization aims to identify Pareto-optimal solutions given a dataset of designs\nand their objective values. In this work, we propose a preference-guided diffusion model that generates\nPareto-optimal designs by leveraging a classifier-based guidance mechanism. Our guidance classifier is\na preference model trained to predict the probability that one design dominates another, directing the\ndiffusion model toward optimal regions of the design space. Crucially, this preference model generalizes\nbeyond the training distribution, enabling the discovery of Pareto-optimal solutions outside the observed\ndataset. We introduce a novel diversity-aware preference guidance, augmenting Pareto dominance prefer-\nence with diversity criteria. This ensures that generated solutions are optimal and well-distributed across\nthe objective space, a capability absent in prior generative methods for offline multi-objective optimiza-\ntion. We evaluate our approach on various continuous offline multi-objective optimization tasks and find\nthat it consistently outperforms other inverse/generative approaches while remaining competitive with\nforward/surrogate-based optimization methods. Our results highlight the effectiveness of classifier-guided\ndiffusion models in generating diverse and high-quality solutions that approximate the Pareto front well."
  },
  {
    "filename": "Fast_online_node_labeling_with_graph_subsampling_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nFast online node labeling with graph subsampling\nYushen Huang yushen.huang@stonybrook.edu\nDepartment of Computer Science\nStony Brook University\nErtai Luo ertai.luo@stonybroook.edu\nDepartment of Computer Science\nStony Brook University\nReza Babenezhad\nMILA\nYifan Sun yifan.sun@stonybrook.edu\nDepartment of Computer Science\nStony Brook University\n\n=== ABSTRACT ===\n\nLarge data applications rely on storing data in massive, sparse graphs with millions to\ntrillions of nodes. Graph-based methods, such as node prediction, aim for computational\nefficiency regardless of graph size. Techniques like localized approximate personalized page\nrank (APPR) solve sparse linear systems with complexity independent of graph size, but is\nin terms of the maximum node degree, which can be much larger in practice than the aver-\nage node degree for real-world large graphs. In this paper, we consider an online subsampled\nAPPR method , where messages are intentionally dropped at random. We use tools from\ngraph sparsifiers and matrix linear algebra to give approximation bounds on the graph\u2019s\nspectral properties ( O(1/\u03f52)edges), and node classification performance (added O(n\u03f5)over-\nhead)."
  },
  {
    "filename": "MerGen:_Micro-electrode_recording_synthesis_using_a_generative_data-driven_approach_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nMerGen: Micro-electrode recording synthesis\nusing a generative data-driven approach\nThibault Martin1\nPaul Sauleau1,2Claire Haegelen1,3\nPierre Jannin1\n, John S.H. Baxter1\u2217\n1Laboratoire Traitement du Signal et de l\u2019Image (INSERM UMR\n1099), University of Rennes (Rennes, France)\n2Rennes University Hospital Centre (Rennes, France)\n3Lyon University Hospital Centre (Lyon, France)\n\n=== ABSTRACT ===\n\nThe analysis of electrophysiological data is crucial for certain surgical\nprocedures such as deep brain stimulation, which has been adopted for\nthe treatment of a variety of neurological disorders. During the proce-\ndure, auditory analysis of these signals helps the clinical team to infer the\nneuroanatomical location of the stimulation electrode and thus optimize\nclinical outcomes. This task is complex, and requires an expert who in\nturn requires significant training. In this paper, we propose a generative\nneural network, called MerGen, capable of simulating de novo electro-\nphysiological recordings, with a view to providing a realistic learning tool\nfor clinicians trainees for identifying these signals. We demonstrate that\nthe generated signals are perceptually indistinguishable from real signals\nby experts in the field, and that it is even possible to condition the gen-\neration efficiently to provide a didactic simulator adapted to a particular\nsurgical scenario. The efficacy of this conditioning is demonstrated, com-\nparing it to intra-observer and inter-observer variability amongst experts.\nWe also demonstrate the use of this network for data augmentation for\nautomatic signal classification which can play a role in decision-making\nsupport in the operating theatre."
  },
  {
    "filename": "A_Language_Anchor-Guided_Method_for_Robust_Noisy_Domain_Generalization_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nA Language Anchor-Guided Method for Robust Noisy\nDomain Generalization\nZilin Daia, Lehong Wangd, Fangzhou Linb,c, Yidong Wangg, Zhigang Lif,\nKazunori D Yamadae, Ziming Zhangb, Wang Luf,\u2217\naDepartment of Computer Science, Worcester Polytechnic\nInstitute, Worcester, 01609, MA, USA\nbDepartment of Electrical Engineering, Worcester Polytechnic\nInstitute, Worcester, 01609, MA, USA\ncDepartment of Robotics Engineering, Worcester Polytechnic\nInstitute, Worcester, 01609, MA, USA\ndCarnegie Mellon University, Pittsburgh, 15213, PA, USA\neTohoku University, Sendai, 980-8572, Japan\nfTsinghua University, Beijing, 100190, China\ngPeking University, Beijing, 100871, China\n\n=== ABSTRACT ===\n\nReal-world machine learning applications are often hindered by two crit-\nical challenges: distribution shift and label noise. Networks inherently tend\nto overfit to redundant, uninformative features present in the training distri-\nbution, which undermines their ability to generalize effectively to the target\ndomain\u2019s distribution. The presence of noisy data further exacerbates this issue\nby inducing additional overfitting to noise, causing existing domain generaliza-\ntion methods to fail in effectively distinguishing invariant features from spurious\nones. To address these challenges, we propose Anchor Alignment and Adaptive\nWeighting ( A3W), a novel algorithm based on sample reweighting guided by\nnatural language processing (NLP) anchors that seeks to extract representa-\ntive features. In particular, A3Wleverages semantic representations derived\nfrom natural language models to serve as a source of domain-invariant prior\nknowledge. We also introduce a weighted loss function that dynamically ad-\njusts the contribution of each sample based on its distance to the corresponding\nNLP anchor, thereby improving the model\u2019s resilience to noisy labels. Extensive\nexperiments on benchmark datasets demonstrate that A3Woutperforms state-\nof-the-art domain generalization methods, yielding significant improvements in\nboth accuracy and robustness across various datasets and noise levels."
  },
  {
    "filename": "Not_Only_Text:_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language_Models_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nNot Only Text: Exploring Compositionality of Visual Representations\nin Vision-Language Models\nDavide Berasi1Matteo Farina2Massimiliano Mancini2\nElisa Ricci1,2Nicola Strisciuglio3\n1Fondazione Bruno Kessler2University of Trento3University of Twente\n\n=== ABSTRACT ===\n\nVision-Language Models (VLMs) learn a shared feature\nspace for text and images, enabling the comparison of in-\nputs of different modalities. While prior works demon-\nstrated that VLMs organize natural language representa-\ntions into regular structures encoding composite meanings,\nit remains unclear if compositional patterns also emerge\nin the visual embedding space. In this work, we investi-\ngate compositionality in the image domain, where the anal-\nysis of compositional properties is challenged by noise and\nsparsity of visual data. We address these problems and\npropose a framework, called Geodesically Decomposable\nEmbeddings (GDE), that approximates image representa-\ntions with geometry-aware compositional structures in the\nlatent space. We demonstrate that visual embeddings of pre-\ntrained VLMs exhibit a compositional arrangement, and\nevaluate the effectiveness of this property in the tasks of\ncompositional classification and group robustness. GDE\nachieves stronger performance in compositional classifica-\ntion compared to its counterpart method that assumes lin-\near geometry of the latent space. Notably, it is particularly\neffective for group robustness, where we achieve higher re-\nsults than task-specific solutions. Our results indicate that\nVLMs can automatically develop a human-like form of com-\npositional reasoning in the visual domain, making their un-\nderlying processes more interpretable. Code is available\nathttps://github.com/BerasiDavide/vlm_image_\ncompositionality ."
  },
  {
    "filename": "Sparse_Logit_Sampling:_Accelerating_Knowledge_Distillation_in_LLMs_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nSparse Logit Sampling: Accelerating Knowledge Distillation in LLMs\nAnshumann* and Mohd Abbas Zaidi* and Akhil Kedia* and Jinwoo Ahn and Taehwak Kwon\nKangwook Lee and Haejun Lee and Joohyung Lee\nSamsung Research, Seoul\n{anshu.mann, abbas.zaidi, akhil.kedia, jinwoo.ahn, taehwak.kwon}@samsung.com\n\n=== ABSTRACT ===\n\nKnowledge distillation can be a cost-effective\ntechnique to distill knowledge in Large Lan-\nguage Models, if the teacher output logits can\nbe pre-computed and cached. However, suc-\ncessfully applying this to pre-training remains\nlargely unexplored. In this work, we prove that\nnaive approaches for sparse knowledge distilla-\ntion such as caching Top-K probabilities, while\nintuitive, provide biased estimates of teacher\nprobability distribution to the student, resulting\nin suboptimal performance and calibration. We\npropose an importance-sampling-based method\n\u2018Random Sampling Knowledge Distillation\u2019,\nwhich provides unbiased estimates, preserves\nthe gradient in expectation, and requires stor-\ning significantly sparser logits. Our method\nenables faster training of student models with\nmarginal overhead ( <10%) compared to cross-\nentropy based training, while maintaining com-\npetitive performance compared to full distilla-\ntion, across a range of model sizes from 300M\nto3B."
  },
  {
    "filename": "Malliavin-Bismut_Score-based_Diffusion_Models_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nMalliavin-Bismut Score-based Diffusion Models\nEhsan Mirafzali1, Utkarsh Gupta1, Patrick Wyrod1\nFrank Proske2,Daniele Venturi1,Razvan Marinescu1\n1University of California, Santa Cruz,2University of Oslo\n{smirafza,utgupta,pwyrod,venturi,ramarine}@ucsc.edu ,\nproske@math.uio.no\n\n=== ABSTRACT ===\n\nWe introduce a new framework that employs Malliavin calculus to derive explicit\nexpressions for the score function\u2014i.e., the gradient of the log-density\u2014associated\nwith solutions to stochastic differential equations (SDEs). Our approach integrates\nclassical integration-by-parts techniques with modern tools, such as Bismut\u2019s for-\nmula and Malliavin calculus, to address linear and nonlinear SDEs. In doing\nso, we establish a rigorous connection between the Malliavin derivative, its ad-\njoint (the Malliavin divergence or the Skorokhod integral), Bismut\u2019s formula, and\ndiffusion generative models, thus providing a systematic method for computing\n\u2207logpt(x). For the linear case, we present a detailed study proving that our\nformula is equivalent to the actual score function derived from the solution of the\nFokker\u2013Planck equation for linear SDEs. Additionally, we derive a closed-form\nexpression for \u2207logpt(x)for nonlinear SDEs with state-independent diffusion\ncoefficients. These advancements provide fresh theoretical insights into the smooth-\nness and structure of probability densities and practical implications for score-based\ngenerative modelling, including the design and analysis of new diffusion models.\nMoreover, our findings promote the adoption of the robust Malliavin calculus\nframework in machine learning research. These results directly apply to various\npure and applied mathematics fields, such as generative modelling, the study of\nSDEs driven by fractional Brownian motion, and the Fokker\u2013Planck equations\nassociated with nonlinear SDEs."
  },
  {
    "filename": "Multi-Span_Optical_Power_Spectrum_Evolution_Modeling_using_ML-based_Multi-Decoder_Attention_Framework_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nMulti-Span Optical Power Spectrum Evolution Modeling using\nML-based Multi-Decoder Attention Framework\nAgastya Raj*,(1), Zehao Wang(2), Frank Slyne(1), Tingjun Chen(2), Dan Kilper(1), Marco Ruffini(1)\n(1)CONNECT Centre, School of Computer Science and Statistics and School of Engineering, Trinity\nCollege Dublin, Ireland *rajag@tcd.ie\n(2)Duke University, Department of Electrical and Computer Engineering, Durham, NC, USA\n\n=== ABSTRACT ===\n\nWe implement a ML-based attention framework with component-specific decoders, improving\noptical power spectrum prediction in multi-span networks. By reducing the need for in-depth training on\neach component, the framework can be scaled to multi-span topologies with minimal data collection,\nmaking it suitable for brown-field scenarios. \u00a92024 The Author(s)"
  },
  {
    "filename": "A_Tale_of_Two_Classes:_Adapting_Supervised_Contrastive_Learning_to_Binary_Imbalanced_Datasets_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nA Tale of Two Classes:\nAdapting Supervised Contrastive Learning to Binary Imbalanced Datasets\nDavid Mildenberger1,2,*, Paul Hager1,*, Daniel Rueckert1,2,3, Martin J. Menten1,2,3\n1Technical University of Munich,2Munich Center for Machine Learning,3Imperial College London\n{david.mildenberger, paul.hager, daniel.rueckert, martin.menten }@tum.de\n*These authors contributed equally.\n\n=== ABSTRACT ===\n\nSupervised contrastive learning (SupCon) has proven to be\na powerful alternative to the standard cross-entropy loss\nfor classification of multi-class balanced datasets. How-\never, it struggles to learn well-conditioned representations\nof datasets with long-tailed class distributions. This prob-\nlem is potentially exacerbated for binary imbalanced dis-\ntributions, which are commonly encountered during many\nreal-world problems such as medical diagnosis. In experi-\nments on seven binary datasets of natural and medical im-\nages, we show that the performance of SupCon decreases\nwith increasing class imbalance. To substantiate these find-\nings, we introduce two novel metrics that evaluate the qual-\nity of the learned representation space. By measuring the\nclass distribution in local neighborhoods, we are able to\nuncover structural deficiencies of the representation space\nthat classical metrics cannot detect. Informed by these in-\nsights, we propose two new supervised contrastive learning\nstrategies tailored to binary imbalanced datasets that im-\nprove the structure of the representation space and increase\ndownstream classification accuracy over standard SupCon\nby up to 35%. We make our code available.1"
  },
  {
    "filename": "Generative_adversarial_framework_to_calibrate_excursion_set_models_for_the_3D_morphology_of_all-solid-state_battery_cathodes_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nGenerative adversarial framework to calibrate excursion set models for\nthe 3D morphology of all-solid-state battery cathodes\nOrkun Furat1,\u2020,\u2217, Sabrina Weber1,\u2020, Johannes Schubert2, Ren\u00b4 e Rekers2, Maximilian Luczak3,\nErik Glatt3, Andreas Wiegmann3, J\u00a8 urgen Janek2, Anja Bielefeld2, Volker Schmidt1\n1Institute of Stochastics, Ulm University, Helmholtzstra\u00dfe 18, 89069 Ulm, Germany\n2Center for Materials Research (ZfM), Justus Liebig University Giessen, Heinrich-Buff-Ring 16, Giessen 35392,\nGermany\n3Math2Market GmbH, Richard-Wagner-Stra\u00dfe 1, 67655 Kaiserslautern, Germany\n\n=== ABSTRACT ===\n\nThis paper presents a computational method for generating virtual 3D morphologies of functional materials using\nlow-parametric stochastic geometry models, i.e., digital twins, calibrated with 2D microscopy images. These digital\ntwins allow systematic parameter variations to simulate various morphologies, that can be deployed for virtual\nmaterials testing by means of spatially resolved numerical simulations of macroscopic properties. Generative\nadversarial networks (GANs) have gained popularity for calibrating models to generate realistic 3D morphologies.\nHowever, GANs often comprise of numerous uninterpretable parameters make systematic variation of morphologies\nfor virtual materials testing challenging. In contrast, low-parametric stochastic geometry models ( e.g., based on\nGaussian random fields) enable targeted variation but may struggle to mimic complex morphologies. Combining\nGANs with advanced stochastic geometry models ( e.g., excursion sets of more general random fields) addresses\nthese limitations, allowing model calibration solely from 2D image data. This approach is demonstrated by\ngenerating a digital twin of all-solid-state battery (ASSB) cathodes. Since the digital twins are parametric, they\nsupport systematic exploration of structural scenarios and their macroscopic properties. The proposed method\nfacilitates simulation studies for optimizing 3D morphologies, benefiting not only ASSB cathodes but also other\nmaterials with similar structures."
  },
  {
    "filename": "Specifying_What_You_Know_or_Not_for_Multi-Label_Class-Incremental_Learning_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nSpecifying What You Know or Not for Multi-Label Class-Incremental Learning\nAoting Zhang1,3, Dongbao Yang1,3*, Chang Liu5, Xiaopeng Hong4*, Yu Zhou2\n1Institute of Information Engineering, Chinese Academy of Sciences\n2VCIP & TMCC & DISSec, College of Computer Science, Nankai University\n3School of Cyber Security, University of Chinese Academy of Sciences\n4Harbin Institute of Technology\n5Tsinghua University\n{zhangaoting, yangdongbao }@iie.ac.cn, liuchang2022@tsinghua.edu.cn\nhongxiaopeng@ieee.org, yzhou@nankai.edu.cn\n\n=== ABSTRACT ===\n\nExisting class incremental learning is mainly designed for\nsingle-label classification task, which is ill-equipped for\nmulti-label scenarios due to the inherent contradiction of\nlearning objectives for samples with incomplete labels. We\nargue that the main challenge to overcome this contradic-\ntion in multi-label class-incremental learning (MLCIL) lies\nin the model\u2019s inability to clearly distinguish between known\nand unknown knowledge. This ambiguity hinders the model\u2019s\nability to retain historical knowledge, master current classes,\nand prepare for future learning simultaneously. In this paper,\nwe target at specifying what is known or not to accommodate\nHistorical, Current, and Prospective knowledge for MLCIL\nand propose a novel framework termed as HCP. Specifically,\n(i) we clarify the known classes by dynamic feature purifica-\ntion and recall enhancement with distribution prior, enhanc-\ning the precision and retention of known information. (ii) We\ndesign prospective knowledge mining to probe the unknown,\npreparing the model for future learning. Extensive experi-\nments validate that our method effectively alleviates catas-\ntrophic forgetting in MLCIL, surpassing the previous state-\nof-the-art by 3.3% on average accuracy for MS-COCO B0-\nC10 setting without replay buffers."
  },
  {
    "filename": "Symbolic_Audio_Classification_via_Modal_Decision_Tree_Learning_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nITADATA2024: The 3rdItalian Conference on Big Data and Data Science\nSymbolic Audio Classification via Modal Decision\nTree Learning\nEnrico Marzano1[0009\u22120001\u22121469\u22126497],\nGiovanni Pagliarini2[0000\u22120002\u22128403\u22123250],\nRiccardo Pasini2[0009\u22120009\u22122807\u22122245],\nGuido Sciavicco2[0000\u22120002\u22129221\u2212879X], and\nIonel Eduard Stan3[0000\u22120001\u22129260\u2212102X]\n1R&D Deparment, Gap S.r.l.u.\nUdine, Italy\n2Applied Computational Logic and Artificial Intelligence (ACLAI) Lab,\nDepartment of Mathematics and Computer Science, University of Ferrara\nFerrara, Italy\n3Database Systems Group, Faculty of Engineering, Free University of Bozen-Bolzano\nBolzano, Italy\n\n=== ABSTRACT ===\n\n. The range of potential applications of acoustic analysis is\nwide. Classification of sounds, in particular, is a typical machine learning\ntask that received a lot of attention in recent years. The most common\napproaches to sound classification are subsymbolic, typically based on\nneural networks, and result in black-box models with high performances\nbut very low transparency. In this work, we consider several audio tasks,\nnamely, age and gender recognition, emotion classification, and respira-\ntory disease diagnosis, and we approach them with a symbolic technique,\nthat is, (modal) decision tree learning. We prove that such tasks can be\nsolved using the same symbolic pipeline, that allows to extract simple\nrules with very high accuracy and low complexity. In principle, all such\ntasks could be associated to an autonomous conversation system, which\ncould be useful in different contexts, such as an automatic reservation\nagent for an hospital or a clinic."
  },
  {
    "filename": "EarlyStopping:_Implicit_Regularization_for_Iterative_Learning_Procedures_in_Python_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nEarlyStopping: Implicit Regularization for Iterative Learning\nProcedures in Python\nEric Ziebell1, Ratmir Miftachov1,2, Bernhard Stankewitz3, Laura Hucker1\n1Institute of Mathematics, Humboldt-Universit\u00a8 at zu Berlin\n2School of Business and Economics, Humboldt-Universit\u00a8 at zu Berlin\n3Institute of Mathematics, Universit\u00a8 at Potsdam\nMarch 24, 2025\n\n=== ABSTRACT ===\n\nIterative learning procedures are ubiquitous in machine learning and modern statistics.\nRegularision is typically required to prevent inflating the expected loss of a procedure in\nlater iterations via the propagation of noise inherent in the data. Significant emphasis has\nbeen placed on achieving this regularisation implicitly by stopping procedures early. The\nEarlyStopping-package provides a toolbox of (in-sample) sequential early stopping rules\nfor several well-known iterative estimation procedures, such as truncated SVD, Landwe-\nber (gradient descent), conjugate gradient descent, L2-boosting and regression trees. One\nof the central features of the package is that the algorithms allow the specification of the\ntrue data-generating process and keep track of relevant theoretical quantities. In this pa-\nper, we detail the principles governing the implementation of the EarlyStopping -package\nand provide a survey of recent foundational advances in the theoretical literature. We\ndemonstrate how to use the EarlyStopping -package to explore core features of implicit\nregularisation and replicate results from the literature."
  },
  {
    "filename": "Curriculum_RL_meets_Monte_Carlo_Planning:_Optimization_of_a_Real_World_Container_Management_Problem_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nCurriculum RL meets Monte Carlo Planning:\nOptimization of a Real World Container\nManagement Problem\nAbhijeet Pendyala /envel\u2322peand Tobias Glasmachers\nRuhr-University Bochum, Bochum, Germany\nfirstname.lastname@ini.rub.de\n\n=== ABSTRACT ===\n\n. In this work, we augment reinforcement learning with an\ninference-time collision model to ensure safe and efficient container man-\nagementinawaste-sortingfacilitywithlimitedprocessingcapacity.Each\ncontainerhastwooptimalemptyingvolumesthattradeoffhigherthrough-\nput against overflow risk. Conventional reinforcement learning (RL) ap-\nproaches struggle under delayed rewards, sparse critical events, and high-\ndimensional uncertainty\u2014failing to consistently balance higher-volume\nempties with the risk of safety-limit violations. To address these chal-\nlenges,weproposeahybridmethodcomprising:(1)a curriculum-learning\npipeline that incrementally trains a PPO agent to handle delayed re-\nwards and class imbalance, and (2) an offline pairwise collision model\nused at inference time to proactively avert collisions with minimal online\ncost. Experimental results show that our targeted inference-time colli-\nsion checks significantly improve collision avoidance, reduce safety-limit\nviolations, maintain high throughput, and scale effectively across vary-\ning container-to-PU ratios. These findings offer actionable guidelines for\ndesigning safe and efficient container-management systems in real-world\nfacilities."
  },
  {
    "filename": "TRACE:_Time_SeRies_PArameter_EffiCient_FinE-tuning_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nTRACE: T ime SeR ies PA rameter EffiC ient\nFinE -tuning\nYuze Li1a, Wei Zhub\naShenzhen International Graduate School, Tsinghua University, Shenzhen, China\nbSchool of Science, University of Hong Kong, Hong Kong, China\n\n=== ABSTRACT ===\n\nWe propose an efficient fine-tuning method for time series foundation models,\ntermed TRACE: Time Series Parameter Efficient Fine-tuning. While pre-\ntrained time series foundation models are gaining popularity, they face the\nfollowing challenges: (1) Unlike natural language tasks, time series data vary\nin frequency, channel numbers, historical/prediction lengths. For long-term\nforecasting tasks in particular, tailored fine-tuning can significantly enhance\nperformance.(2) Existing parameter-efficient tuning methods like LoRA re-\nmain applicable but require adaptation to temporal characteristics.\nTo address these challenges, our TRACE framework introduces two key\ninnovations: (1) Gated DSIC (Gated Dynamic Simulation Importance Cal-\nculation), an unbiased LoRA module importance selection mechanism that\nensures conditional parameter consistency before and after masking. Ex-\nperiments demonstrate that Gated DSIC outperforms common fine-tuning.\n(2) Reconstructed prediction heads for long-term forecasting tasks, which\nachieve comparable or superior performance to linear probing heads while\ndrastically reducing parameter counts.\nExtensive experiments on long-/short-term forecasting and anomaly de-\ntection tasks across diverse datasets, coupled with ablation studies, validate\nthe effectiveness of our method."
  },
  {
    "filename": "Uncertainty-Driven_Modeling_of_Microporosity_and_Permeability_in_Clastic_Reservoirs_Using_Random_Forest_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nmanuscript submitted to Natural Resources Research  \nMarch 4th, 2025  \n1 \n \n Uncertainty -Driven Modeling of Microporosity and Permeability in Clastic \nReservoirs Using Random Forest  \nMuhammad Risha1*, Mohamed Elsaadany2, Paul Liu1 \n1North Carolina State University, Marine, Earth and Atmospheric Sciences Department, Raleigh, NC 27695, USA  \n2Geoscience Department, Universiti Teknologi PETRONAS, Seri Iskandar 32610, Perak, Malaysia\n\n=== ABSTRACT ===\n\nPredicting microporosity and permeability in clastic \nreservoirs is a challenge in reservoir quality \nassessment, especially in formations where direct \nmeasurements are difficult or expensive. These \nreservoir properties are fundamental in determining \na reser voir's capacity for fluid storage and \ntransmission, yet conventional methods for \nevaluating them, such as Mercury Injection \nCapillary Pressure (MICP) and Scanning Electron \nMicroscopy (SEM), are resource -intensive. The aim \nof this study is  to develop a cost-effective machine \nlearning model to predict complex reservoir \nproperties using readily available field data and \nbasic laboratory analyses. A Random Forest \nclassifier was employed, utilizing key geological \nparameters such as porosity, grain size distri bution, \nand spectral gamma -ray (SGR) measurements. An \nuncertainty analysis was applied to account for \nnatural variability, expanding the dataset, and \nenhancing the model's robustness. The model \nachieved a high level of accuracy in predicting \nmicroporosity (93%) and permeability levels (88%). \nBy using easily obtainable data, this model reduces \nthe reliance on expensive laboratory methods, \nmaking it a valuable tool for early -stage exploration, \nespecially in remote or offshore environments. The \nintegration of machine learning with uncertainty \nanalysis provides a reliable and cost -effective \napproach for evaluating key reservoir properties in \nsiliciclastic formations. This model offers a practical \nsolution to improve reservoir quality assessments, \nenabling more i nformed decision -making and \noptimizing exploration efforts."
  },
  {
    "filename": "Robustness_of_deep_learning_classification_to_adversarial_input_on_GPUs:_asynchronous_parallel_accumulation_is_a_source_of_vulnerability_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nRobustness of deep learning classification to\nadversarial input on GPUs: asynchronous parallel\naccumulation is a source of vulnerability\nSanjif Shanmugavelu1, Mathieu Taillefumier2, Christopher Culver1, Vijay\nGanesh3, Oscar Hernandez4, and Ada Sedova4\n1Groq Inc.,2ETH Zurich/CSCS,3Georgia Institute of Technology,3Oak Ridge\nNational Laboratory\nsshanmugavelu@groq.com, tmathieu@ethz.ch, sedovaaa@ornl.gov\n\n=== ABSTRACT ===\n\n. The ability of machine learning (ML) classification models\nto resist small, targeted input perturbations\u2014known as adversarial at-\ntacks\u2014is a key measure of their safety and reliability. We show that\nfloating-point non associativity (FPNA) coupled with asynchronous par-\nallel programming on GPUs is sufficient to result in misclassification,\nwithout any perturbation to the input. Additionally, we show this mis-\nclassification is particularly significant for inputs close to the decision\nboundary and that standard adversarial robustness results may be over-\nestimated up to 4.6% when not considering machine-level details. We\nfirst study a linear classifier, before focusing on standard Graph Neural\nNetwork (GNN) architectures and datasets used in robustness assess-\nments. We present a novel black-box attack using Bayesian optimization\nto determine external workloads that bias the output of reductions on\nGPUs and reliably lead to misclassification. Motivated by these results,\nwe present a new learnable permutation (LP) gradient-based approach,\nto learn floating point operation orderings that lead to misclassifications,\nmaking the assumption that any reduction or permutation ordering is\npossible. This LP approach provides a worst-case estimate in a compu-\ntationally efficient manner, avoiding the need to run identical experi-\nments tens of thousands of times over a potentially large set of possible\nGPU states or architectures. Finally, we investigate parallel reduction\nordering across different GPU architectures for a reduction under three\nconditions: (1) executing external background workloads, (2) utilizing\nmulti-GPU virtualization, and (3) applying power capping. Our results\ndemonstrate that parallel reduction ordering varies significantly across\narchitectures under the first two conditions. These results and the meth-\nods developed here can help to include machine-level considerations into\nadversarial robustness assessments, which can make a difference in safety\nand mission critical applications."
  },
  {
    "filename": "Online_Selective_Conformal_Prediction:_Errors_and_Solutions_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nOnline Selective Conformal Prediction:\nErrors and Solutions\nYusuf Sale1,4and Aaditya Ramdas2,3\n1Institute of Computer Science, Ludwig-Maximilians Universit\u00a8 at M\u00a8 unchen\n2Department of Statistics and Data Science, Carnegie Mellon University\n3Machine Learning Department, Carnegie Mellon University\n4Munich Center for Machine Learning (MCML)\n\n=== ABSTRACT ===\n\nIn online selective conformal inference, data arrives sequentially, and prediction intervals are con-\nstructed only when an online selection rule is met. Since online selections may break the exchange-\nability between the selected test datum and the rest of the data, one must correct for this by suitably\nselecting the calibration data. In this paper, we evaluate existing calibration selection strategies and\npinpoint some fundamental errors in the associated claims that guarantee selection-conditional cov-\nerage and control of the false coverage rate (FCR). To address these shortcomings, we propose novel\ncalibration selection strategies that provably preserve the exchangeability of the calibration data and\nthe selected test datum. Consequently, we demonstrate that online selective conformal inference with\nthese strategies guarantees both selection-conditional coverage and FCR control. Our theoretical\nfindings are supported by experimental evidence examining tradeoffs between valid methods."
  },
  {
    "filename": "Model-free_front-to-end_training_of_a_large_high_performance_laser_neural_network_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nModel-free front-to-end training of a large high\nperformance laser neural network\nAnas Skalli1*, Satoshi Sunada2, Mirko Goldmann1,\nMarcin Gebski3, Stephan Reitzenstein4, James A. Lott4,\nTomasz Czyszanowski3, Daniel Brunner1\n1*Institut FEMTO-ST, Universit\u00b4 e Marie et Louis Pasteur, CNRS UMR,\n6174, Besan\u00b8 con, France.\n2Faculty of Mechanical Engineering, Institute of Science and\nEngineering, Kanazawa University, Kakuma-machi Kanazawa, Ishikawa\n920\u20131192, Japan.\n3Institute of Physics, Lodz University of Technology, ul. W\u00b4 olczanska\n219, 90-924 Lodz, Poland.\n4Technical University of Berlin, Hardenbergstra\u00dfe 36, D-10623 Berlin,\nGermany.\n*Corresponding author(s). E-mail(s): anas.skalli@femto-st.fr;\nanasskalli2@gmail.com;\n\n=== ABSTRACT ===\n\nArtificial neural networks (ANNs), have become ubiquitous and revolutionized\nmany applications ranging from computer vision to medical diagnoses. However,\nthey offer a fundamentally connectionist and distributed approach to computing,\nin stark contrast to classical computers that use the von Neumann architecture.\nThis distinction has sparked renewed interest in developing unconventional hard-\nware to support more efficient implementations of ANNs, rather than merely\nemulating them on traditional systems. Photonics stands out as a particularly\npromising platform, providing scalability, high speed, energy efficiency, and the\nability for parallel information processing. However, fully realized autonomous\noptical neural networks (ONNs) with in-situ learning capabilities are still rare.\nIn this work, we demonstrate a fully autonomous and parallel ONN using a\nmultimode vertical cavity surface emitting laser (VCSEL) using off-the-shelf\ncomponents. Our ONN is highly efficient and is scalable both in network size\nand inference bandwidth towards the GHz range. High performance hardware-\ncompatible optimization algorithms are necessary in order to minimize reliance\n1arXiv:2503.16943v1  [cs.LG]  21 Mar 2025\non external von Neumann computers to fully exploit the potential of ONNs. As\nsuch we present and extensively study several algorithms which are broadly com-\npatible with a wide range of systems. We then apply these algorithms to optimize\nour ONN, and benchmark them using the MNIST dataset. We show that our\nONN can achieve high accuracy and convergence efficiency, even under limited\nhardware resources. Crucially, we compare these different algorithms in terms of\nscaling and optimization efficiency in term of convergence time which is crucial\nwhen working with limited external resources. Our work provides some guidance\nfor the design of future ONNs as well as a simple and flexible way to train them."
  },
  {
    "filename": "Preferential_Multi-Objective_Bayesian_Optimization_for_Drug_Discovery_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nPreferential Multi-Objective Bayesian Optimization for Drug Discovery\nTai Dang* 1 2 3Long-Hung Pham* 4Sang T. Truong* 2Ari Glenn3Wendy Nguyen1Edward A. Pham3\nJeffrey S. Glenn3Sanmi Koyejo2Thang Luong1\n\n=== ABSTRACT ===\n\nDespite decades of advancements in automated\nligand screening, large-scale drug discovery\nremains resource-intensive and requires post-\nprocessing hit selection, a step where chemists\nmanually select a few promising molecules based\non their chemical intuition. This creates a ma-\njor bottleneck in the virtual screening process for\ndrug discovery, demanding experts to repeatedly\nbalance complex trade-offs among drug properties\nacross a vast pool of candidates. To improve the\nefficiency and reliability of this process, we pro-\npose a novel human-centered framework named\nCheapVS that allows chemists to guide the lig-\nand selection process by providing preferences re-\ngarding the trade-offs between drug properties via\npairwise comparison. Our framework combines\npreferential multi-objective Bayesian optimiza-\ntion with a docking model for measuring bind-\ning affinity to capture human chemical intuition\nfor improving hit identification. Specifically, on\na library of 100K chemical candidates targeting\nEGFR and DRD2, CheapVS outperforms state-\nof-the-art screening methods in identifying drugs\nwithin a limited computational budget. Notably,\nour method can recover up to 16/37 EGFR and\n37/58 DRD2 known drugs while screening only\n6% of the library, showcasing its potential to sig-\nnificantly advance drug discovery."
  },
  {
    "filename": "Assessing_Consistency_and_Reproducibility_in_the_Outputs_of_Large_Language_Models:_Evidence_Across_Diverse_Finance_and_Accounting_Tasks_front_matter.txt",
    "content": "=== FRONT MATTER ===\n\nAssessing C onsistency  and Reproducibility  in the Outputs of Large Language Models: \nEvidence Across Diverse Finance and Accounting Tasks  \n \nMarch  2025 \n \nJulian Junyan Wang  \nUniversity College, University of Oxford \njulian.wang@univ.ox.ac.uk   \n \nVictor Xiaoqi Wang  \nCollege of Business, California State University Long Beach \nvictor.wang@csulb.edu\n\n=== ABSTRACT ===\n\n: This study provides the first comprehensive assessment of consistency and reproducibility in \nLarge Language Model (LLM) outputs in  finance and accounting research. We evaluate how consistently \nLLMs produce outputs given identical inputs through extensive experimentation with 50 independent runs \nacross five common tasks: classification, sentiment analysis, summarization, text generation, and prediction. Using three OpenAI models ( GPT-3.5-turbo, GPT -4o-mini, and GPT -4o), we generate over 3.4 \nmillion  outputs from diverse financial source texts and data, covering MD&As, FOMC statements, finance \nnews articles, earnings call  transcripts, and financial statements . Our findings reveal substantial but task -\ndependent consistency, with binary classification and sentiment analysis achieving near -perfect \nreproducibility, while complex tasks show greater variability. More advanced models do not consistently \ndemonstrate b etter consistency and reproducibility, with task -specific patterns emerging. LLMs \nsignificantl y outperform expert  human annotators in consistency and maintain high agreement even where \nhuman experts significantly disagree. We further find that s imple aggregation strategies across 3 -5 runs \ndramatically improve consistency. Simulation analysis reveals that despite measurable inconsistency  in \nLLM outputs , downstream statistical inferences remain remarkably robust. These findings address concerns \nabout what we term \"G-hacking, \" the selective reporting of favorable outcomes from multiple Generative \nAI runs, by demonstrating that such risks are relatively low for finance and accounting tasks. \n \nKeywords : Generative AI (GenAI), Large Language Models (LLMs), ChatGPT, Reproducibility, \nG-hacking  \n \n \n1 \n Assessing C onsistency  and Reproducibility in the Outputs of Large Language Models: \nEvidence Across Diverse Finance and Accounting Tasks"
  }
]