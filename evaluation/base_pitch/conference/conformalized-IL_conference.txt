Abstract: Conformalized Interactive Imitation Learning: An Algorithmic Approach to Address Expert Shift & Intermittent Feedback

Interactive Imitation Learning (IL) presents a research challenge in effectively quantifying uncertainty during distribution shifts in deployment. Prior uncertainty quantification methodologies, such as ensemble disagreement or Monte Carlo dropout, have demonstrated limitations, often resulting in overconfident estimates during distribution shifts. This paper presents a novel approach to uncertainty quantification in IL, leveraging conformal prediction and introducing an algorithm, Intermittent Quantile Tracking (IQT), which utilizes a probabilistic model of intermittent labels. This innovative algorithm provides asymptotic coverage guarantees and empirical coverage levels. A new method, ConformalDAgger, is also introduced, which employs prediction intervals calibrated by IQT to actively solicit expert feedback during deployment. Comparative experimental results for ConformalDAgger versus traditional uncertainty-aware DAgger methods, during both simulated and hardware deployments on a 7DOF robotic manipulator, demonstrate superior performance in detecting high uncertainty and facilitating faster learning of new behaviours.