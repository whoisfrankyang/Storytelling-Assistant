Abstract: Re-evaluating the Mixture-of-Agents Paradigm: A Comprehensive Examination of Large Language Model Ensembles

Recent advancements in ensemble methodologies have underscored the potential of Mixture-of-Agents (MoA) strategies, which amalgamate outputs from an array of Large Language Models (LLMs). This research rigorously interrogates the efficacy of such approaches, introducing Self-MoA, an innovative technique that exclusively amalgamates outputs from the highest performing LLM. Results from exhaustive experimentation indicate that Self-MoA consistently surpasses conventional MoA, securing a 6.6% improvement on the AlpacaEval 2.0 benchmark and an average 3.8% enhancement across various benchmarks, such as MMLU, CRUX, and MATH. The application of Self-MoA on a top-tier model in AlpacaEval 2.0 resulted in record-breaking performance. This research meticulously analyses the trade-off between diversity and output quality, and suggests that the performance of MoA is significantly influenced by quality, with the blending of diverse LLMs often detrimentally affecting average quality. We also outline the circumstances where mixed LLMs may prove advantageous. Furthermore, we introduce a sequential variant of Self-MoA, capable of efficiently amalgamating a plethora of LLM outputs in real-time across multiple rounds, with equivalent efficacy to single-round aggregation.