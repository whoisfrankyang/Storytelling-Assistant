This research asks if combining different large language models (like AI that can write or read) really improves their performance. We found that using just the best-performing model often works better than mixing different ones. This method, which we call Self-MoA, improved performance by up to 6.6% in our tests, even beating the top scores on some benchmarks. So, it seems that quality is more important than diversity when it comes to these models. However, we also found some situations where using different models can be helpful. We also developed a version of Self-MoA that can handle lots of model outputs quickly and effectively.