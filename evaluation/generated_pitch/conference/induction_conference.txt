"In our study, we explore the effectiveness of different learning paradigms - induction, which identifies latent functions from examples, and transduction, which predicts test outputs directly from inputs. We conducted our investigation in the context of the Abstract Reasoning Corpus (ARC), using neural models trained on synthetic variations of Python programs. Our findings suggest that while inductive program synthesis excels at precise computations and concept composition, transduction was more successful with perceptual concepts. By combining these two approaches, we achieved performance nearing human levels on ARC. This study identifies future areas of exploration, namely, the optimal integration of inductive and transductive models for superior performance in machine learning tasks."