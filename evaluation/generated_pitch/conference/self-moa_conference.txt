'''This research challenges the efficacy of the Mixture-of-Agents (MoA) ensemble method in Large Language Models (LLMs), positing whether the amalgamation of various LLMs truly enhances performance. We introduce a novel approach, Self-MoA, which uniquely harnesses one top-performing LLM, achieving a 6.6% improvement over MoA in AlpacaEval 2.0 and setting new performance records. Further analysis reveals a trade-off between diversity and quality in MoA, with an overemphasis on diversity often compromising quality. However, we also identify potential scenarios where mixing of LLMs could be beneficial. Our study opens up new avenues for efficient ensemble methods in LLMs, promising significant implications for future AI applications.'''