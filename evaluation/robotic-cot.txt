Robotic Control via Embodied Chain-of-Thought Reasoning

Abstract: A key limitation of learned robot control policies is their inability to
generalize outside their training data. Recent works on vision-language-action models
(VLAs) have shown that the use of large, internet pre-trained vision-language
models as the backbone of learned robot policies can substantially improve their
robustness and generalization ability. Yet, one of the most exciting capabilities of
large vision-language models in other domains is their ability to reason iteratively
through complex problems. Can that same capability be brought into robotics
to allow policies to improve performance by reasoning about a given task before
acting? Naive use of “chain-of-thought” (CoT) style prompting is significantly less
effective with standard VLAs because of the relatively simple training examples
that are available to them. Additionally, the purely-semantic reasoning about subtasks
common to regular CoT is insufficient for robot policies that need to ground
their reasoning in sensory observations and the robot state. To this end, we introduce
Embodied Chain-of-Thought Reasoning (ECoT) for VLAs, in which we train
VLAs to perform multiple steps of reasoning about plans, sub-tasks, motions, and
visually grounded features like object bounding boxes and end effector positions,
before predicting the robot action. We design a scalable pipeline for generating
synthetic training data for ECoT on large robot datasets. We demonstrate that ECoT
increases the absolute success rate of OpenVLA, the current strongest open-source
VLA policy, by 28% across challenging generalization tasks without any additional
robot training data. Additionally, ECoT makes it easier for humans to interpret
a policy’s failures and correct its behavior interactively using natural language.
Finally, we show that our model learns to transfer ECoT reasonings to unseen
embodiments and tasks.